[
  {
    "id": "omg-cable",
    "title": "OMG Cable",
    "slug": "omg-cable",
    "html": "<p>The USB Rubber Ducky was what first drew my attention to Hak5, but I just couldn't pay the price they wanted for one.  I went the open source way and created a Bad USB using a Raspberry Pi Pico.  The OMG cable was something that I am not advanced enough to create on my own.  It is one of the stealthiest tools that I have seen.  When I received it, I realized I had another cable that looked exactly the same.  It is a good thing Hak5 sent a little orange clip to place on the cable so that I could tell them apart.</p><p>Upon opening the cable, I used the OMG Programmer and the Web Flasher to flash the firmware and make the cable usable.  Afterwards I grabbed an extra Windows laptop and plugged the OMG Cable in.  By default, the OMG cable acts as a hotspot creating its own network.  After connecting to the OMG network, I opened a web browser and went to http://192.168.4.1.  I was greeted with the OMG cable's web console.</p><figure class=\"kg-card kg-image-card\"><img src=\"__GHOST_URL__/content/images/2025/11/image.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"1054\" srcset=\"__GHOST_URL__/content/images/size/w600/2025/11/image.png 600w, __GHOST_URL__/content/images/size/w1000/2025/11/image.png 1000w, __GHOST_URL__/content/images/size/w1600/2025/11/image.png 1600w, __GHOST_URL__/content/images/size/w2400/2025/11/image.png 2400w\" sizes=\"(min-width: 720px) 720px\"></figure><p>There is a payload menu that allows you to create and save 50 different payloads and save one as a Bootscript that runs when the OMG cable comes online.  The payloads are written in an enhanced version of DuckyScript and there is a <a href=\"https://github.com/hak5/omg-payloads\" rel=\"noreferrer\">repository</a> with several pre-built payloads.</p><figure class=\"kg-card kg-image-card\"><img src=\"__GHOST_URL__/content/images/2025/11/image-1.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1565\" height=\"1159\" srcset=\"__GHOST_URL__/content/images/size/w600/2025/11/image-1.png 600w, __GHOST_URL__/content/images/size/w1000/2025/11/image-1.png 1000w, __GHOST_URL__/content/images/2025/11/image-1.png 1565w\" sizes=\"(min-width: 720px) 720px\"></figure><p>Going to the Settings, you have the option to change the OMG cables Wi-Fi settings.  You can change the SSID, password, channel, and MAC address or you can change the OMG cable to connect to your existing Wi-Fi.  There is also an option to connect to an OMG C2 server, but it is currently in beta.  The OMG C2 allows you to connect to your OMG cable over the internet.</p><figure class=\"kg-card kg-image-card\"><img src=\"__GHOST_URL__/content/images/2025/11/image-2.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"319\" height=\"837\"></figure><p>The USB settings allow you to configure hardware settings for the OMG cable.  They also allow you to configure the Keylog and HIDX settings.  If a keyboard is connected to a computer using the OMG cable, the OMG cable can act as a keylogger as well.  It also allows you to use the OMG cable as a mouse jiggler.  This helped me during testing by ensuring the host machine didn't go into sleep mode.</p><figure class=\"kg-card kg-image-card\"><img src=\"__GHOST_URL__/content/images/2025/11/image-3.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"313\" height=\"1227\"></figure>",
    "plaintext": "The USB Rubber Ducky was what first drew my attention to Hak5, but I just couldn't pay the price they wanted for one. I went the open source way and created a Bad USB using a Raspberry Pi Pico. The OMG cable was something that I am not advanced enough to create on my own. It is one of the stealthiest tools that I have seen. When I received it, I realized I had another cable that looked exactly the same. It is a good thing Hak5 sent a little orange clip to place on the cable so that I could tell them apart.\n\nUpon opening the cable, I used the OMG Programmer and the Web Flasher to flash the firmware and make the cable usable. Afterwards I grabbed an extra Windows laptop and plugged the OMG Cable in. By default, the OMG cable acts as a hotspot creating its own network. After connecting to the OMG network, I opened a web browser and went to http://192.168.4.1. I was greeted with the OMG cable's web console.\n\nThere is a payload menu that allows you to create and save 50 different payloads and save one as a Bootscript that runs when the OMG cable comes online. The payloads are written in an enhanced version of DuckyScript and there is a repository with several pre-built payloads.\n\nGoing to the Settings, you have the option to change the OMG cables Wi-Fi settings. You can change the SSID, password, channel, and MAC address or you can change the OMG cable to connect to your existing Wi-Fi. There is also an option to connect to an OMG C2 server, but it is currently in beta. The OMG C2 allows you to connect to your OMG cable over the internet.\n\nThe USB settings allow you to configure hardware settings for the OMG cable. They also allow you to configure the Keylog and HIDX settings. If a keyboard is connected to a computer using the OMG cable, the OMG cable can act as a keylogger as well. It also allows you to use the OMG cable as a mouse jiggler. This helped me during testing by ensuring the host machine didn't go into sleep mode.",
    "feature_image": null,
    "excerpt": "The USB Rubber Ducky was what first drew my attention to Hak5, but I just couldn't pay the price they wanted for one. I went the open source way and created a Bad USB using a Raspberry Pi Pico. The OM...",
    "published_at": "2025-11-22T22:42:14.000Z",
    "published_date": "November 22, 2025",
    "tags": [
      "Cyber Security"
    ],
    "featured": false
  },
  {
    "id": "proxmox-virtual-environment",
    "title": "Proxmox Virtual Environment",
    "slug": "proxmox-virtual-environment",
    "html": "<p>Proxmox Virtual Environment (VE) is an open-source server management platform designed for enterprise virtualization. Based on Debian GNU/Linux, it allows you to easily manage virtual machines (VMs) and containers from a single, centralized web-based interface. It integrates two powerful virtualization technologies: KVM (Kernel-based Virtual Machine) for managing heavyweight, full-hardware virtual machines, and LXC (Linux Containers) for lightweight, operating-system-level virtualization.</p><p>This combination makes Proxmox incredibly flexible, suitable for both large-scale enterprise deployments and small home labs. Key features include the ability to cluster multiple Proxmox servers together, enabling high availability (HA) for critical workloads and live migration of running VMs between physical hosts without downtime. It also has robust, built-in support for various storage solutions, including local storage like ZFS and networked or distributed storage like Ceph, making it a comprehensive and cost-effective solution.</p><p>Using Proxmox helped me to create complex labs that I would have struggle to build without a virtualization platform.  I had been using VirtualBox before I switch to Proxmox.  I made the switch because I wanted to host my VMs on a separate machine.  Doing so allowed me to dedicate all of my system resources to virtualization, except for the Proxmox overhead (which is basically nothing).</p><figure class=\"kg-card kg-image-card kg-width-wide\"><img src=\"__GHOST_URL__/content/images/2025/10/proxmox-2.webp\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"1037\" srcset=\"__GHOST_URL__/content/images/size/w600/2025/10/proxmox-2.webp 600w, __GHOST_URL__/content/images/size/w1000/2025/10/proxmox-2.webp 1000w, __GHOST_URL__/content/images/size/w1600/2025/10/proxmox-2.webp 1600w, __GHOST_URL__/content/images/size/w2400/2025/10/proxmox-2.webp 2400w\" sizes=\"(min-width: 1200px) 1200px\"></figure><p>I have created so many virtual machines and Linux containers in Proxmox that I have the process memorized.  I have tried almost every Linux flavor and MacOS.  I spent the first few months just experimenting with different operating systems and configurations in Promxox.</p><p>After getting used to the platform, I purchased three Intel NUCS and create a CEPH cluster with high-availability.  This was a lot easier process than I thought it would be, but after a few months I realize that it was overkill for what I wanted to deploy and I would have more resources if I split the cluster up.</p><p>After getting a few virtual machines populated with services and data that was important to me, I started looking at backup solutions.  The first backup solution that I used was an NFS share from my Synology.  This worked fairly well, but it did not offer any form of incremental backup or deduplication.  The setup that I ended up going with was to run a Proxmox Backup Server virtual machine on my Synology.  This allowed me to use the larger hard drives of my NAS, but also to have access to the advanced features of Proxmox Backup Server.</p><h3 id=\"key-advantages-of-proxmox-backup-server-pbs\">Key Advantages of Proxmox Backup Server (PBS)</h3><ul><li><strong>Deduplication &amp; Incremental Backups</strong> This is the biggest reason. PBS doesn't create full backups every time.<ul><li><strong>Incremental:</strong> After the <em>first</em> full backup, it only saves the blocks of data that have changed. This is much faster and uses far less network bandwidth.</li><li><strong>Deduplication:</strong> PBS is \"content-aware.\" It breaks backups into small, unique chunks. If you have 10 VMs running the same operating system, PBS stores the shared OS files <strong>only once</strong>. This results in enormous storage savings. A 100GB VM backed up daily for a week might only consume 110GB, not 700GB.</li></ul></li><li><strong>Strong Encryption (Client-Side)</strong> PBS encrypts your backup data <strong>on the Proxmox host</strong> <em>before</em> it's sent over the network to the backup server. This means your backup data is secure both in transit and at rest, even if the backup server itself isn't fully trusted. Backups to a standard NFS share are typically unencrypted.</li><li><strong>Data Integrity &amp; Verification</strong> PBS uses built-in checksums to ensure your data hasn't been corrupted (bit rot). You can schedule regular <strong>verification jobs</strong> to read back your backups and confirm they are intact and restorable. An NFS share provides no such guarantee; you won't know a backup file is bad until you try to restore it.</li><li><strong>Centralized Management</strong> PBS provides a clean web interface to manage all your backups, retention policies (e.g., \"keep 7 daily, 4 weekly, 3 monthly\"), verification schedules, and storage status from a single dashboard. Managing <code>vzdump</code> files on an NFS share is a manual process of deleting old files.</li><li><strong>File-Level Restore</strong> With PBS, you can browse the file system of a VM backup directly from the GUI and restore individual files or folders without having to restore the entire virtual machine. This is incredibly useful for quick recoveries.</li></ul><p>After discovering all of these features there was no way that I could run a Proxmox instance without Proxmox Backup Server.  It has helped me to restore not only entire virtual machines, but to pull files off an old virtual machine and restore them to a different one.</p><p>The next feature of Proxmox that I wanted to look into was a way to track virtual machine metrics.  The Proxmox webui provides limited stats about the virtual machines and does not keep historical data.  It does provide a built in way to send metrics to a metric server, such as InfluxDB or OpenTelemetry.  I chose to use InfluxDB because I had already had it setup.  It only took me about 10 minutes to connect Proxmox to InfluxDB and the InfluxDB to Grafana.  Afterwards, I had a nice dashboard that display several stats and could provide historical data.  You can tell InfluxDB to keep your data for as long as you want when you create the storage bucket, but I went with 30 days.</p><figure class=\"kg-card kg-image-card\"><img src=\"__GHOST_URL__/content/images/2025/10/grafana.webp\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1106\" height=\"591\" srcset=\"__GHOST_URL__/content/images/size/w600/2025/10/grafana.webp 600w, __GHOST_URL__/content/images/size/w1000/2025/10/grafana.webp 1000w, __GHOST_URL__/content/images/2025/10/grafana.webp 1106w\" sizes=\"(min-width: 720px) 720px\"></figure><p>Then I wanted a way to passthrough a GPU so that I could use it on a virtual machine running Ollama.  The actual passthrough of the device was easy enough. You just go to Resource Mappings under the Datacenter view of the Proxmox webui and add a PCI Device.  The more difficult part at first was figuring out how to download the Nvidia driver and get them to work correctly.  After figuring out how to blacklist the default Nvidia drivers and download the correct ones, everything was up and running.  Then a few weeks later, I made the mistake of using apt to update and upgrade everything, including the Nvidia drivers.  This caused a mismatch in the driver and the library version.  A had to force remove everything related to Nvidia and start over at that point.  After this fiasco, I made sure to hold Nvidia driver version in apt.</p>",
    "plaintext": "Proxmox Virtual Environment (VE) is an open-source server management platform designed for enterprise virtualization. Based on Debian GNU/Linux, it allows you to easily manage virtual machines (VMs) and containers from a single, centralized web-based interface. It integrates two powerful virtualization technologies: KVM (Kernel-based Virtual Machine) for managing heavyweight, full-hardware virtual machines, and LXC (Linux Containers) for lightweight, operating-system-level virtualization.\n\nThis combination makes Proxmox incredibly flexible, suitable for both large-scale enterprise deployments and small home labs. Key features include the ability to cluster multiple Proxmox servers together, enabling high availability (HA) for critical workloads and live migration of running VMs between physical hosts without downtime. It also has robust, built-in support for various storage solutions, including local storage like ZFS and networked or distributed storage like Ceph, making it a comprehensive and cost-effective solution.\n\nUsing Proxmox helped me to create complex labs that I would have struggle to build without a virtualization platform. I had been using VirtualBox before I switch to Proxmox. I made the switch because I wanted to host my VMs on a separate machine. Doing so allowed me to dedicate all of my system resources to virtualization, except for the Proxmox overhead (which is basically nothing).\n\nI have created so many virtual machines and Linux containers in Proxmox that I have the process memorized. I have tried almost every Linux flavor and MacOS. I spent the first few months just experimenting with different operating systems and configurations in Promxox.\n\nAfter getting used to the platform, I purchased three Intel NUCS and create a CEPH cluster with high-availability. This was a lot easier process than I thought it would be, but after a few months I realize that it was overkill for what I wanted to deploy and I would have more resources if I split the cluster up.\n\nAfter getting a few virtual machines populated with services and data that was important to me, I started looking at backup solutions. The first backup solution that I used was an NFS share from my Synology. This worked fairly well, but it did not offer any form of incremental backup or deduplication. The setup that I ended up going with was to run a Proxmox Backup Server virtual machine on my Synology. This allowed me to use the larger hard drives of my NAS, but also to have access to the advanced features of Proxmox Backup Server.\n\n\nKey Advantages of Proxmox Backup Server (PBS)\n\n * Deduplication & Incremental Backups This is the biggest reason. PBS doesn't create full backups every time.\n   * Incremental: After the first full backup, it only saves the blocks of data that have changed. This is much faster and uses far less network bandwidth.\n   * Deduplication: PBS is \"content-aware.\" It breaks backups into small, unique chunks. If you have 10 VMs running the same operating system, PBS stores the shared OS files only once. This results in enormous storage savings. A 100GB VM backed up daily for a week might only consume 110GB, not 700GB.\n * Strong Encryption (Client-Side) PBS encrypts your backup data on the Proxmox host before it's sent over the network to the backup server. This means your backup data is secure both in transit and at rest, even if the backup server itself isn't fully trusted. Backups to a standard NFS share are typically unencrypted.\n * Data Integrity & Verification PBS uses built-in checksums to ensure your data hasn't been corrupted (bit rot). You can schedule regular verification jobs to read back your backups and confirm they are intact and restorable. An NFS share provides no such guarantee; you won't know a backup file is bad until you try to restore it.\n * Centralized Management PBS provides a clean web interface to manage all your backups, retention policies (e.g., \"keep 7 daily, 4 weekly, 3 monthly\"), verification schedules, and storage status from a single dashboard. Managing vzdump files on an NFS share is a manual process of deleting old files.\n * File-Level Restore With PBS, you can browse the file system of a VM backup directly from the GUI and restore individual files or folders without having to restore the entire virtual machine. This is incredibly useful for quick recoveries.\n\nAfter discovering all of these features there was no way that I could run a Proxmox instance without Proxmox Backup Server. It has helped me to restore not only entire virtual machines, but to pull files off an old virtual machine and restore them to a different one.\n\nThe next feature of Proxmox that I wanted to look into was a way to track virtual machine metrics. The Proxmox webui provides limited stats about the virtual machines and does not keep historical data. It does provide a built in way to send metrics to a metric server, such as InfluxDB or OpenTelemetry. I chose to use InfluxDB because I had already had it setup. It only took me about 10 minutes to connect Proxmox to InfluxDB and the InfluxDB to Grafana. Afterwards, I had a nice dashboard that display several stats and could provide historical data. You can tell InfluxDB to keep your data for as long as you want when you create the storage bucket, but I went with 30 days.\n\nThen I wanted a way to passthrough a GPU so that I could use it on a virtual machine running Ollama. The actual passthrough of the device was easy enough. You just go to Resource Mappings under the Datacenter view of the Proxmox webui and add a PCI Device. The more difficult part at first was figuring out how to download the Nvidia driver and get them to work correctly. After figuring out how to blacklist the default Nvidia drivers and download the correct ones, everything was up and running. Then a few weeks later, I made the mistake of using apt to update and upgrade everything, including the Nvidia drivers. This caused a mismatch in the driver and the library version. A had to force remove everything related to Nvidia and start over at that point. After this fiasco, I made sure to hold Nvidia driver version in apt.",
    "feature_image": null,
    "excerpt": "Proxmox Virtual Environment (VE) is an open-source server management platform designed for enterprise virtualization. Based on Debian GNU/Linux, it allows you to easily manage virtual machines (VMs) a...",
    "published_at": "2025-10-21T14:51:44.000Z",
    "published_date": "October 21, 2025",
    "tags": [
      "Cyber Security"
    ],
    "featured": false
  },
  {
    "id": "wazuh",
    "title": "Wazuh",
    "slug": "wazuh",
    "html": "<p>Wazuh is a free and open-source security platform that unifies XDR (Extended Detection and Response) and SIEM (Security Information and Event Management) capabilities. It is designed to provide comprehensive threat prevention, detection, and response for workloads across on-premises, virtualized, containerized, and cloud environments. The platform operates using a central management server that collects, aggregates, and analyzes data from lightweight agents installed on monitored endpoints, such as laptops, servers, and cloud instances. This architecture allows security teams to gain deep visibility into their infrastructure and protect it from a wide range of cyber threats.</p><p>The core power of Wazuh lies in its extensive set of integrated security features. It performs real-time log data analysis, intrusion detection, and file integrity monitoring to identify suspicious activities and policy violations. The platform also includes robust vulnerability detection and security configuration assessment (SCA), which scan for known weaknesses and misconfigurations. By combining these capabilities with incident response actions and dashboards for regulatory compliance (such as PCI DSS, HIPAA, and GDPR), Wazuh provides a powerful, all-in-one solution for security operations and threat hunting.</p><p>My first introduction to Wazuh was an earlier version of Security Onion.  At the time, I knew what it did for Security Onion, but I did not realize all of the features that the Wazuh platform by itself had.  When I first installed it, I planned on using it as just a SIEM.  Something that I could send logs to from my lab to get a better understanding of logs and incident identification.</p><p>When I loaded the dashboard for the first time, it took me a minute to find the menu for adding agents, but once I did, it was a simple process to add more.  Wazuh produces the commands for install on the operating system that you select and then you just copy and paste them into the host system.  </p><figure class=\"kg-card kg-image-card\"><img src=\"__GHOST_URL__/content/images/2025/10/image-1.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1429\" height=\"707\" srcset=\"__GHOST_URL__/content/images/size/w600/2025/10/image-1.png 600w, __GHOST_URL__/content/images/size/w1000/2025/10/image-1.png 1000w, __GHOST_URL__/content/images/2025/10/image-1.png 1429w\" sizes=\"(min-width: 720px) 720px\"></figure><p>Once the agents are added, the overview dashboard begins to populate with information about alerts from the last 24 hours and display the agents connection status.  It also has links to several other areas of the site.</p><figure class=\"kg-card kg-image-card\"><img src=\"__GHOST_URL__/content/images/2025/10/image-2.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1424\" height=\"675\" srcset=\"__GHOST_URL__/content/images/size/w600/2025/10/image-2.png 600w, __GHOST_URL__/content/images/size/w1000/2025/10/image-2.png 1000w, __GHOST_URL__/content/images/2025/10/image-2.png 1424w\" sizes=\"(min-width: 720px) 720px\"></figure><p>The first thing that stood out to me was the Configuration Assessment.  Since I wanted to learn not only how to secure systems, but also how to attack systems this was a great place for me to start.  Once you select the agent that you want to view stats on it will run a series of test from the CIS Benchmarks for that operating system.  I ran it against a Windows Server 2022 with a default configuration.  This was a fresh install that was promoted to domain controller and had the file server feature added.  I was surprised by the score.</p><figure class=\"kg-card kg-image-card\"><img src=\"__GHOST_URL__/content/images/2025/10/image-4.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1425\" height=\"655\" srcset=\"__GHOST_URL__/content/images/size/w600/2025/10/image-4.png 600w, __GHOST_URL__/content/images/size/w1000/2025/10/image-4.png 1000w, __GHOST_URL__/content/images/2025/10/image-4.png 1425w\" sizes=\"(min-width: 720px) 720px\"></figure><p>The default configuration only met 30% of the CIS benchmarks.  I think that the most powerful feature of this screen is when you start looking into the individual checks themselves.  Not only does it provide a description, but it details the reasoning behind the control and how to fix the misconfiguration.</p><figure class=\"kg-card kg-image-card\"><img src=\"__GHOST_URL__/content/images/2025/10/image-5.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"943\" height=\"677\" srcset=\"__GHOST_URL__/content/images/size/w600/2025/10/image-5.png 600w, __GHOST_URL__/content/images/2025/10/image-5.png 943w\" sizes=\"(min-width: 720px) 720px\"></figure><p>After running through a couple of the configuration, I wanted to see what else Wazuh could do.  I went to the main dashboard for the same endpoint and found even more topics of interest.  It displayed the event count for the endpoint, but it also mapped those events to the MITRE ATT&amp;CK Framework.</p><figure class=\"kg-card kg-image-card\"><img src=\"__GHOST_URL__/content/images/2025/10/image-6.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"932\" height=\"383\" srcset=\"__GHOST_URL__/content/images/size/w600/2025/10/image-6.png 600w, __GHOST_URL__/content/images/2025/10/image-6.png 932w\" sizes=\"(min-width: 720px) 720px\"></figure><p>Upon following the link for the MITRE Framework, I was met with even more information on a separate dashboard.  Which displays a lot of information about how your endpoint logs correlate to the MITRE Framework.</p><figure class=\"kg-card kg-image-card\"><img src=\"__GHOST_URL__/content/images/2025/10/image-7.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1425\" height=\"675\" srcset=\"__GHOST_URL__/content/images/size/w600/2025/10/image-7.png 600w, __GHOST_URL__/content/images/size/w1000/2025/10/image-7.png 1000w, __GHOST_URL__/content/images/2025/10/image-7.png 1425w\" sizes=\"(min-width: 720px) 720px\"></figure><p>I found the real value by going back to the endpoint dashboard and clicking on the specific Tactic. For example Defense Evasion when clicked on produced another menu that mapped events to the specific node in the framework.</p><figure class=\"kg-card kg-image-card\"><img src=\"__GHOST_URL__/content/images/2025/10/image-8.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"208\" height=\"395\"></figure><p>After clicking on the specific node here, I was presented with even more information about the actual event that triggered these nodes, including the actual Windows Event.</p><figure class=\"kg-card kg-image-card\"><img src=\"__GHOST_URL__/content/images/2025/10/image-9.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"855\" height=\"631\" srcset=\"__GHOST_URL__/content/images/size/w600/2025/10/image-9.png 600w, __GHOST_URL__/content/images/2025/10/image-9.png 855w\" sizes=\"(min-width: 720px) 720px\"></figure><p>Then I went browsing into the File Integrity Monitoring dashboard for the same endpoint and was greeted with another display that quickly showed statistics.</p><figure class=\"kg-card kg-image-card\"><img src=\"__GHOST_URL__/content/images/2025/10/image-10.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1422\" height=\"721\" srcset=\"__GHOST_URL__/content/images/size/w600/2025/10/image-10.png 600w, __GHOST_URL__/content/images/size/w1000/2025/10/image-10.png 1000w, __GHOST_URL__/content/images/2025/10/image-10.png 1422w\" sizes=\"(min-width: 720px) 720px\"></figure><p>Going to the Events tab allowed me to see the events that were used to populate the dashboard and clicking on an individual event allowed me to see the relevant information for that event.</p><figure class=\"kg-card kg-image-card\"><img src=\"__GHOST_URL__/content/images/2025/10/image-11.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1424\" height=\"674\" srcset=\"__GHOST_URL__/content/images/size/w600/2025/10/image-11.png 600w, __GHOST_URL__/content/images/size/w1000/2025/10/image-11.png 1000w, __GHOST_URL__/content/images/2025/10/image-11.png 1424w\" sizes=\"(min-width: 720px) 720px\"></figure><figure class=\"kg-card kg-image-card\"><img src=\"__GHOST_URL__/content/images/2025/10/image-12.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"702\" height=\"551\" srcset=\"__GHOST_URL__/content/images/size/w600/2025/10/image-12.png 600w, __GHOST_URL__/content/images/2025/10/image-12.png 702w\"></figure><p>I was starting to see how powerful the Wazuh platform was now and I had not made it halfway through yet.  The next section that I went to was the Vulnerability Detection dashboard for the same endpoint.  Again, I was immediately greeted with a clear view of relevant information, including the number of vulnerabilities, the severity, and the top CVEs associated with these vulnerabilities.</p><figure class=\"kg-card kg-image-card\"><img src=\"__GHOST_URL__/content/images/2025/10/image-13.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1433\" height=\"686\" srcset=\"__GHOST_URL__/content/images/size/w600/2025/10/image-13.png 600w, __GHOST_URL__/content/images/size/w1000/2025/10/image-13.png 1000w, __GHOST_URL__/content/images/2025/10/image-13.png 1433w\" sizes=\"(min-width: 720px) 720px\"></figure><p>The inventory tab of this section provided me with a clear view of those vulnerabilities with a description and a link to the associated CVE.</p><figure class=\"kg-card kg-image-card\"><img src=\"__GHOST_URL__/content/images/2025/10/image-14.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1419\" height=\"625\" srcset=\"__GHOST_URL__/content/images/size/w600/2025/10/image-14.png 600w, __GHOST_URL__/content/images/size/w1000/2025/10/image-14.png 1000w, __GHOST_URL__/content/images/2025/10/image-14.png 1419w\" sizes=\"(min-width: 720px) 720px\"></figure><p>Most of these vulnerabilities were fixed in the most recent patch Tuesday from Microsoft, so a quick update of the server fixed them.</p><p>I had spent several hours learning this platform at this point and there is still so much more for me to learn.  It also provides a way to interact with Docker and cloud environments such as AWS.  All of the information that I discovered in Wazuh was all based on the default detection rules as well.  I mature security team could use this platform and customize the rules and alerts to their organization.</p>",
    "plaintext": "Wazuh is a free and open-source security platform that unifies XDR (Extended Detection and Response) and SIEM (Security Information and Event Management) capabilities. It is designed to provide comprehensive threat prevention, detection, and response for workloads across on-premises, virtualized, containerized, and cloud environments. The platform operates using a central management server that collects, aggregates, and analyzes data from lightweight agents installed on monitored endpoints, such as laptops, servers, and cloud instances. This architecture allows security teams to gain deep visibility into their infrastructure and protect it from a wide range of cyber threats.\n\nThe core power of Wazuh lies in its extensive set of integrated security features. It performs real-time log data analysis, intrusion detection, and file integrity monitoring to identify suspicious activities and policy violations. The platform also includes robust vulnerability detection and security configuration assessment (SCA), which scan for known weaknesses and misconfigurations. By combining these capabilities with incident response actions and dashboards for regulatory compliance (such as PCI DSS, HIPAA, and GDPR), Wazuh provides a powerful, all-in-one solution for security operations and threat hunting.\n\nMy first introduction to Wazuh was an earlier version of Security Onion. At the time, I knew what it did for Security Onion, but I did not realize all of the features that the Wazuh platform by itself had. When I first installed it, I planned on using it as just a SIEM. Something that I could send logs to from my lab to get a better understanding of logs and incident identification.\n\nWhen I loaded the dashboard for the first time, it took me a minute to find the menu for adding agents, but once I did, it was a simple process to add more. Wazuh produces the commands for install on the operating system that you select and then you just copy and paste them into the host system.\n\nOnce the agents are added, the overview dashboard begins to populate with information about alerts from the last 24 hours and display the agents connection status. It also has links to several other areas of the site.\n\nThe first thing that stood out to me was the Configuration Assessment. Since I wanted to learn not only how to secure systems, but also how to attack systems this was a great place for me to start. Once you select the agent that you want to view stats on it will run a series of test from the CIS Benchmarks for that operating system. I ran it against a Windows Server 2022 with a default configuration. This was a fresh install that was promoted to domain controller and had the file server feature added. I was surprised by the score.\n\nThe default configuration only met 30% of the CIS benchmarks. I think that the most powerful feature of this screen is when you start looking into the individual checks themselves. Not only does it provide a description, but it details the reasoning behind the control and how to fix the misconfiguration.\n\nAfter running through a couple of the configuration, I wanted to see what else Wazuh could do. I went to the main dashboard for the same endpoint and found even more topics of interest. It displayed the event count for the endpoint, but it also mapped those events to the MITRE ATT&CK Framework.\n\nUpon following the link for the MITRE Framework, I was met with even more information on a separate dashboard. Which displays a lot of information about how your endpoint logs correlate to the MITRE Framework.\n\nI found the real value by going back to the endpoint dashboard and clicking on the specific Tactic. For example Defense Evasion when clicked on produced another menu that mapped events to the specific node in the framework.\n\nAfter clicking on the specific node here, I was presented with even more information about the actual event that triggered these nodes, including the actual Windows Event.\n\nThen I went browsing into the File Integrity Monitoring dashboard for the same endpoint and was greeted with another display that quickly showed statistics.\n\nGoing to the Events tab allowed me to see the events that were used to populate the dashboard and clicking on an individual event allowed me to see the relevant information for that event.\n\nI was starting to see how powerful the Wazuh platform was now and I had not made it halfway through yet. The next section that I went to was the Vulnerability Detection dashboard for the same endpoint. Again, I was immediately greeted with a clear view of relevant information, including the number of vulnerabilities, the severity, and the top CVEs associated with these vulnerabilities.\n\nThe inventory tab of this section provided me with a clear view of those vulnerabilities with a description and a link to the associated CVE.\n\nMost of these vulnerabilities were fixed in the most recent patch Tuesday from Microsoft, so a quick update of the server fixed them.\n\nI had spent several hours learning this platform at this point and there is still so much more for me to learn. It also provides a way to interact with Docker and cloud environments such as AWS. All of the information that I discovered in Wazuh was all based on the default detection rules as well. I mature security team could use this platform and customize the rules and alerts to their organization.",
    "feature_image": null,
    "excerpt": "Wazuh is a free and open-source security platform that unifies XDR (Extended Detection and Response) and SIEM (Security Information and Event Management) capabilities. It is designed to provide compre...",
    "published_at": "2025-10-20T19:57:18.000Z",
    "published_date": "October 20, 2025",
    "tags": [
      "Cyber Security"
    ],
    "featured": false
  },
  {
    "id": "wireshark",
    "title": "Wireshark",
    "slug": "wireshark",
    "html": "<p>I began using Wireshark at work a few months ago to help troubleshoot network issues.  The first time I used it was to diagnose some DHCP issues.  It took me about five minutes to figure out that someone had plugged in another Wi-Fi router thinking it would help with the internet speed in their group home.</p><p>Since then I have been playing around with it at home trying to learn all I can about the invisible part of networking.  The first time I started it up at home, I thought something was terribly wrong.  I have a Pangolin instance on a VPS and use it to route traffic to some internal services.  It has a wildcard domain DNS entry so that I can spin up and down different service without making DNS changes.  Once I stop experimenting with different self-hosted service, I am going to manually enter each host, but it is nice not to have to create and delete entries several times just to try a container or service out.</p><p>So I started Wireshark up and the first thing that jumped out at me were these entries:</p><figure class=\"kg-card kg-image-card kg-width-full\"><img src=\"__GHOST_URL__/content/images/2025/10/image.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1482\" height=\"243\" srcset=\"__GHOST_URL__/content/images/size/w600/2025/10/image.png 600w, __GHOST_URL__/content/images/size/w1000/2025/10/image.png 1000w, __GHOST_URL__/content/images/2025/10/image.png 1482w\"></figure><p>I don't have a service that uses the hostname wpad, so the first thing that came to my mind was that something nefarious was going on.  After a recent cyber incident at work, I have more of an understanding that cyber criminals don't care about the size of a target.  For some criminals, it is all a numbers game.</p><p>Then after some research, I discovered that it was not anything nefarious.  It is a feature that Windows enables by default called Web Proxy Auto-Discovery (wpad). Wpad is a method used by web browsers and other applications to find and automatically use web proxies.  The host machine first asks the DHCP server for a wpad option, but since my DHCP server is not configured with this option, it falls back to DNS.</p><p>  Since I have a wildcard domain, the DNS query comes back with the IP address for my Pangolin VPS.  It is a valid DNS entry so the host machine tries to connect to it only to have the connection refused by Pangolin or specifically the Traefik proxy docker container on the VPS.  </p><p>After discovering this, the first thing I did was the easiest.  I turned off this setting on my computer.  Then I thought about all the other computers on my home network and decided to make some more centralized changes. I did the secure thing that I should have done from the beginning and enter specific host names for any services exposed through Pangolin.  I should have done this from the beginning, since I have an internal Traefik server as well, but I was trying to move away from having two Docker servers. </p><p>Now I will keep the internal one and use my DNS server to sinkhole this host so that I can still use an internal wildcard.  I have already removed the wildcard entry that I had entered on my Cloudflare DNS and only exposed a few services with specific host names.</p><p>This was just the first rabbit hole that I went down using Wireshark.  It is a useful tool for anything network related.  There is so much information available that I can spend the next few months just diving into it.  Once I get my network figured out, there are several sample pcap files available for download and analysis at <a href=\"https://wiki.wireshark.org/samplecaptures\">https://wiki.wireshark.org/samplecaptures</a> and eventually I plan on taking the certification offered by Wireshark, the Wireshark Certified Analyst (WCA).  I don't feel that I am ready for it yet, but I do know that I am motivated achieve it.  It would be a nice addition to my certifications and allow me to show competency in a universally known network tool with an endless amount of use cases.</p>",
    "plaintext": "I began using Wireshark at work a few months ago to help troubleshoot network issues. The first time I used it was to diagnose some DHCP issues. It took me about five minutes to figure out that someone had plugged in another Wi-Fi router thinking it would help with the internet speed in their group home.\n\nSince then I have been playing around with it at home trying to learn all I can about the invisible part of networking. The first time I started it up at home, I thought something was terribly wrong. I have a Pangolin instance on a VPS and use it to route traffic to some internal services. It has a wildcard domain DNS entry so that I can spin up and down different service without making DNS changes. Once I stop experimenting with different self-hosted service, I am going to manually enter each host, but it is nice not to have to create and delete entries several times just to try a container or service out.\n\nSo I started Wireshark up and the first thing that jumped out at me were these entries:\n\nI don't have a service that uses the hostname wpad, so the first thing that came to my mind was that something nefarious was going on. After a recent cyber incident at work, I have more of an understanding that cyber criminals don't care about the size of a target. For some criminals, it is all a numbers game.\n\nThen after some research, I discovered that it was not anything nefarious. It is a feature that Windows enables by default called Web Proxy Auto-Discovery (wpad). Wpad is a method used by web browsers and other applications to find and automatically use web proxies. The host machine first asks the DHCP server for a wpad option, but since my DHCP server is not configured with this option, it falls back to DNS.\n\nSince I have a wildcard domain, the DNS query comes back with the IP address for my Pangolin VPS. It is a valid DNS entry so the host machine tries to connect to it only to have the connection refused by Pangolin or specifically the Traefik proxy docker container on the VPS.\n\nAfter discovering this, the first thing I did was the easiest. I turned off this setting on my computer. Then I thought about all the other computers on my home network and decided to make some more centralized changes. I did the secure thing that I should have done from the beginning and enter specific host names for any services exposed through Pangolin. I should have done this from the beginning, since I have an internal Traefik server as well, but I was trying to move away from having two Docker servers.\n\nNow I will keep the internal one and use my DNS server to sinkhole this host so that I can still use an internal wildcard. I have already removed the wildcard entry that I had entered on my Cloudflare DNS and only exposed a few services with specific host names.\n\nThis was just the first rabbit hole that I went down using Wireshark. It is a useful tool for anything network related. There is so much information available that I can spend the next few months just diving into it. Once I get my network figured out, there are several sample pcap files available for download and analysis at https://wiki.wireshark.org/samplecaptures and eventually I plan on taking the certification offered by Wireshark, the Wireshark Certified Analyst (WCA). I don't feel that I am ready for it yet, but I do know that I am motivated achieve it. It would be a nice addition to my certifications and allow me to show competency in a universally known network tool with an endless amount of use cases.",
    "feature_image": null,
    "excerpt": "I began using Wireshark at work a few months ago to help troubleshoot network issues. The first time I used it was to diagnose some DHCP issues. It took me about five minutes to figure out that someon...",
    "published_at": "2025-10-20T15:30:31.000Z",
    "published_date": "October 20, 2025",
    "tags": [
      "Cyber Security"
    ],
    "featured": false
  },
  {
    "id": "meshtastic",
    "title": "Meshtastic",
    "slug": "meshtastic",
    "html": "<p>Meshtastic is an open-source, off-grid communication platform that uses inexpensive LoRa (Long Range) radio modules to create decentralized mesh networks. Unlike traditional radios that require direct line-of-sight or cellular networks that depend on infrastructure, Meshtastic devices automatically relay messages through other nodes in the network, extending range significantly. Each radio can communicate several miles in open terrain, but by hopping through multiple devices, messages can travel much further. The system works completely offline with no monthly fees, making it ideal for hiking, camping, emergency preparedness, or any situation where cellular coverage is unreliable or unavailable.</p><p>The project supports various hardware platforms, primarily ESP32-based devices, which can be purchased for around $30-50. Users can communicate via smartphone apps (iOS/Android) over Bluetooth, or use the radios standalone with small OLED screens. Beyond simple text messaging, Meshtastic supports GPS position sharing, creating a live map of all nodes in your mesh network. The community has built additional features like remote sensor monitoring, weather station integration, and even MQTT gateways to bridge multiple mesh networks over the internet. It's become popular with hikers, preppers, amateur radio enthusiasts, and anyone interested in resilient, community-owned communications infrastructure.</p><p>I live in an area that does not have great cell phone coverage everywhere so I bought a pair of these devices for when my family and I a hiking through the woods and loose service.  After purchasing this pair of devices and using them for a few times, I regretted not spending the extra money and getting a pair with a larger screen and a keyboard.  These are bad to use once you get them connected to your cell phone, but sometimes the configuration would not stick and I found myself unable to connect them without resetting the device.</p>",
    "plaintext": "Meshtastic is an open-source, off-grid communication platform that uses inexpensive LoRa (Long Range) radio modules to create decentralized mesh networks. Unlike traditional radios that require direct line-of-sight or cellular networks that depend on infrastructure, Meshtastic devices automatically relay messages through other nodes in the network, extending range significantly. Each radio can communicate several miles in open terrain, but by hopping through multiple devices, messages can travel much further. The system works completely offline with no monthly fees, making it ideal for hiking, camping, emergency preparedness, or any situation where cellular coverage is unreliable or unavailable.\n\nThe project supports various hardware platforms, primarily ESP32-based devices, which can be purchased for around $30-50. Users can communicate via smartphone apps (iOS/Android) over Bluetooth, or use the radios standalone with small OLED screens. Beyond simple text messaging, Meshtastic supports GPS position sharing, creating a live map of all nodes in your mesh network. The community has built additional features like remote sensor monitoring, weather station integration, and even MQTT gateways to bridge multiple mesh networks over the internet. It's become popular with hikers, preppers, amateur radio enthusiasts, and anyone interested in resilient, community-owned communications infrastructure.\n\nI live in an area that does not have great cell phone coverage everywhere so I bought a pair of these devices for when my family and I a hiking through the woods and loose service. After purchasing this pair of devices and using them for a few times, I regretted not spending the extra money and getting a pair with a larger screen and a keyboard. These are bad to use once you get them connected to your cell phone, but sometimes the configuration would not stick and I found myself unable to connect them without resetting the device.",
    "feature_image": null,
    "excerpt": "Meshtastic is an open-source, off-grid communication platform that uses inexpensive LoRa (Long Range) radio modules to create decentralized mesh networks. Unlike traditional radios that require direct...",
    "published_at": "2025-10-19T00:17:18.000Z",
    "published_date": "October 19, 2025",
    "tags": [
      "Home-Lab"
    ],
    "featured": false
  },
  {
    "id": "my-pwnagotchi-aka-bobby",
    "title": "My Pwnagotchi aka Bobby",
    "slug": "my-pwnagotchi-aka-bobby",
    "html": "<p>I recently set up a Pwnagotchi, an AI-powered Wi-Fi security research tool that runs on a Raspberry Pi Zero W. Built by Simone Margaritelli (evilsocket), Pwnagotchi uses deep reinforcement learning to learn from its Wi-Fi environment and autonomously capture WPA handshakes from nearby networks. Think of it as a Tamagotchi, but instead of feeding it, you're training an AI to become better at WiFi reconnaissance. The project displays adorable ASCII faces that change based on its mood and success rate, making security research surprisingly endearing.</p><p>My configuration leverages several community plugins from repositories by jayofelony, Sniffleupagus, and others. I've enabled the grid plugin for sharing statistics with the Pwnagotchi community, along with auto-tune to optimize the AI's learning parameters automatically. The session-stats plugin tracks my device's performance over time, helping me see how well my Pwnagotchi is learning. I'm running it with the web interface enabled for remote management, which makes it easy to check in on its progress and download captured handshakes.</p><p>One interesting aspect of my setup is the memory filesystem optimization, which uses zram to reduce SD card wear by keeping logs and data in compressed RAM. This is crucial for long-term reliability since Raspberry Pi SD cards can wear out quickly with constant writes. I've also configured it to whitelist certain networks and disabled the display output, opting instead to access everything through the web interface. The result is a portable, autonomous security research device that continues learning and improving its Wi-Fi reconnaissance skills wherever I take it.</p><p>The first thing I did once I had a working version on Pwnagotchi was show it to my children.  They thought it was the cutest thing ever until I told them that it was the reason they were getting kicked off the internet.  I plugged it back into the computer and made some changes to the config file.  I changed it's hostname to bobby and whitelisted my home Wi-Fi.  This stopped it from attacking my Wifi network.  I was living in an apartment at the time and thought about turning it loose on the 30 Wi-Fi networks within range.  I was pretty sure that nobody living near me was technical enough to trace it back, but since it is a crime I created a VLAN and set it up to only attack that VLAN by modifying bettercap to specify a specific SSID.</p><p>I really enjoyed this project but the only downside is that I can't ever take bobby outside to play without breaking the law.  </p>",
    "plaintext": "I recently set up a Pwnagotchi, an AI-powered Wi-Fi security research tool that runs on a Raspberry Pi Zero W. Built by Simone Margaritelli (evilsocket), Pwnagotchi uses deep reinforcement learning to learn from its Wi-Fi environment and autonomously capture WPA handshakes from nearby networks. Think of it as a Tamagotchi, but instead of feeding it, you're training an AI to become better at WiFi reconnaissance. The project displays adorable ASCII faces that change based on its mood and success rate, making security research surprisingly endearing.\n\nMy configuration leverages several community plugins from repositories by jayofelony, Sniffleupagus, and others. I've enabled the grid plugin for sharing statistics with the Pwnagotchi community, along with auto-tune to optimize the AI's learning parameters automatically. The session-stats plugin tracks my device's performance over time, helping me see how well my Pwnagotchi is learning. I'm running it with the web interface enabled for remote management, which makes it easy to check in on its progress and download captured handshakes.\n\nOne interesting aspect of my setup is the memory filesystem optimization, which uses zram to reduce SD card wear by keeping logs and data in compressed RAM. This is crucial for long-term reliability since Raspberry Pi SD cards can wear out quickly with constant writes. I've also configured it to whitelist certain networks and disabled the display output, opting instead to access everything through the web interface. The result is a portable, autonomous security research device that continues learning and improving its Wi-Fi reconnaissance skills wherever I take it.\n\nThe first thing I did once I had a working version on Pwnagotchi was show it to my children. They thought it was the cutest thing ever until I told them that it was the reason they were getting kicked off the internet. I plugged it back into the computer and made some changes to the config file. I changed it's hostname to bobby and whitelisted my home Wi-Fi. This stopped it from attacking my Wifi network. I was living in an apartment at the time and thought about turning it loose on the 30 Wi-Fi networks within range. I was pretty sure that nobody living near me was technical enough to trace it back, but since it is a crime I created a VLAN and set it up to only attack that VLAN by modifying bettercap to specify a specific SSID.\n\nI really enjoyed this project but the only downside is that I can't ever take bobby outside to play without breaking the law.",
    "feature_image": null,
    "excerpt": "I recently set up a Pwnagotchi, an AI-powered Wi-Fi security research tool that runs on a Raspberry Pi Zero W. Built by Simone Margaritelli (evilsocket), Pwnagotchi uses deep reinforcement learning to...",
    "published_at": "2025-10-18T22:41:42.000Z",
    "published_date": "October 18, 2025",
    "tags": [
      "Cyber Security"
    ],
    "featured": false
  },
  {
    "id": "raspberry-pi-5",
    "title": "Raspberry Pi 5",
    "slug": "raspberry-pi-5",
    "html": "<h2 id=\"initial-configuration\">Initial Configuration</h2><p>This is a Raspberry Pi 5 that I setup to have access to tools no matter which network I attach it to.  The Raspberry Pi is the 8GB model and I used a PoE hat that included the SSD slot.</p><p>The first thing that I did was use Raspberry Pi Imager and image a nvme SSD with the Lite Raspberry Pi Linux.  I added my SSH settings into the image during the flashing process.  This ensured that once I had the IP address, I could connect remotely to it without every hooking up a monitor or keyboard.</p><p>The second thing was to install Tailscale.  Any VPN solution would work, but I have been using Tailscale for my home lab for a while now and like how easy it is to configure.  After installing Tailscale, the following commands allowed me to connect and expose SSH to my Tailnet (Tailscale network).</p><pre><code>sudo tailscale up --ssh\nsudo systemctl enable tailscaled\nsudo tailscale set --operator=$USER</code></pre><p>The first command connects and exposes SSH.  The second command sets the Tailscale service (tailscaled) to start at boot.  The third command is to allow the current user SSH access.  The default setup for Tailscale is only to allow root SSH access.  Now once I plug the Raspberry Pi into a network and it pulls a DHCP address, I will have a remote connection to it over my Tailnet.  As long as Tailscale would work on the network, I will have SSH access to it.</p><p>Then I add some tools.  I could just flash a Kali image onto the Raspberry Pi, but I don't want the operating system to be detected as Kali.  The first reason is because I will use this box as a troubleshooting tool and I don't want it be be identified as an intrusion.  The second reason is the same, but I don't want it to be fingerprinted if I am doing offensive work.</p><h2 id=\"tools\">Tools</h2><h3 id=\"prerequisites\">Prerequisites</h3><ul><li>Go</li></ul><pre><code>sudo apt install golang-go\necho 'export PATH=$PATH:~/go/bin' &gt;&gt; ~/.bashrc\nsource ~/.bashrc</code></pre><h3 id=\"network-scanning-discovery\">Network Scanning &amp; Discovery</h3><p><strong>nmap</strong> - Industry-standard network scanner for host discovery, port scanning, OS detection, and service enumeration</p><pre><code class=\"language-bash\">sudo apt install nmap</code></pre><p><strong>rustscan</strong> - Blazingly fast modern port scanner that feeds results to nmap</p><pre><code class=\"language-bash\"># Install Rust if you haven't already\ncurl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh\nsource \"$HOME/.cargo/env\"\n\n# Install RustScan via cargo\ncargo install rustscan</code></pre><p><strong>masscan</strong> - Ultra-fast port scanner for large-scale network scanning</p><pre><code class=\"language-bash\">sudo apt install masscan</code></pre><p><strong>arp-scan</strong> - Fast ARP-based local network device discovery</p><pre><code class=\"language-bash\">sudo apt install arp-scan</code></pre><p><strong>fping</strong> - Parallel ICMP echo probe tool for fast host checking</p><pre><code class=\"language-bash\">sudo apt install fping</code></pre><p><strong>hping3</strong> - TCP/IP packet crafting and network testing tool</p><pre><code class=\"language-bash\">sudo apt install hping3</code></pre><p><strong>snmp / snmpwalk</strong> - Query remote systems via SNMP protocol</p><pre><code class=\"language-bash\">sudo apt install snmp snmp-mibs-downloader</code></pre><h3 id=\"network-traffic-analysis-monitoring\">Network Traffic Analysis &amp; Monitoring</h3><p><strong>tcpdump</strong> - Powerful CLI packet capture and analysis tool</p><pre><code class=\"language-bash\">sudo apt install tcpdump</code></pre><p><strong>tshark</strong> - Wireshark's CLI for detailed packet analysis with filtering</p><pre><code class=\"language-bash\">sudo apt install tshark</code></pre><p><strong>bettercap</strong> - Modern network attack and monitoring framework</p><pre><code class=\"language-bash\">sudo apt install bettercap</code></pre><h3 id=\"network-performance-testing\">Network Performance Testing</h3><p><strong>iperf3</strong> - Standard tool for measuring TCP/UDP bandwidth between hosts</p><pre><code class=\"language-bash\">sudo apt install iperf3</code></pre><p><strong>mtr</strong> - Combined ping and traceroute with real-time statistics</p><pre><code class=\"language-bash\">sudo apt install mtr</code></pre><h3 id=\"security-testing-vulnerability-scanning\">Security Testing &amp; Vulnerability Scanning</h3><p><strong>nuclei</strong> - Modern, fast vulnerability scanner with extensive template library</p><pre><code class=\"language-bash\">wget https://github.com/projectdiscovery/nuclei/releases/download/v3.3.6/nuclei_3.3.6_linux_arm64.zip\nunzip nuclei_3.3.6_linux_arm64.zip\nsudo mv nuclei /usr/local/bin/\nsudo chmod +x /usr/local/bin/nuclei</code></pre><p><strong>nikto</strong> - Classic web server vulnerability scanner (older but still useful)</p><pre><code class=\"language-bash\">sudo apt install nikto</code></pre><p><strong>wpscan</strong> - WordPress security scanner and vulnerability database</p><pre><code class=\"language-bash\">sudo apt install ruby ruby-dev libcurl4-openssl-dev make zlib1g-dev\nsudo gem install wpscan</code></pre><p><strong>sqlmap</strong> - Automatic SQL injection detection and exploitation tool</p><pre><code class=\"language-bash\">sudo apt install sqlmap</code></pre><p><strong>hydra</strong> - Fast network login brute-forcer supporting many protocols</p><pre><code class=\"language-bash\">sudo apt install hydra</code></pre><p><strong>ffuf</strong> - Modern, fast web fuzzer for directory/file discovery and fuzzing</p><pre><code class=\"language-bash\">sudo apt install ffuf</code></pre><p><strong>feroxbuster</strong> - Fast, recursive content discovery tool written in Rust</p><pre><code class=\"language-bash\"># Install from releases (not in standard apt)\nwget https://github.com/epi052/feroxbuster/releases/download/v2.10.4/feroxbuster_arm64.deb.zip\nunzip feroxbuster_arm64.deb.zip\nsudo dpkg -i feroxbuster_*_arm64.deb</code></pre><p><strong>gobuster</strong> - Fast directory/file &amp; DNS enumeration tool</p><pre><code class=\"language-bash\">sudo apt install gobuster</code></pre><p><strong>netexec</strong> - Swiss army knife for pentesting networks (replaces CrackMapExec)</p><pre><code class=\"language-bash\"># Install Rust\ncurl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh\n\n# When prompted, choose option 1 (default installation)\n# Then activate Rust in current shell\nsource \"$HOME/.cargo/env\"\n\n\nsudo apt install -y build-essential libssl-dev libffi-dev python3-dev pipx\npipx install git+https://github.com/Pennyw0rth/NetExec</code></pre><p><strong>impacket-scripts</strong> - Essential collection of Python scripts for network protocols</p><pre><code class=\"language-bash\">sudo apt install python3-impacket</code></pre><p><strong>enum4linux-ng</strong> - Modern Python rewrite of enum4linux for Windows/Samba enumeration</p><pre><code class=\"language-bash\">sudo apt install git smbclient python3-ldap3 python3-yaml python3-impacket\ngit clone https://github.com/cddmp/enum4linux-ng.git\ncd enum4linux-ng\nsudo chmod +x enum4linux-ng.py\nsudo cp enum4linux-ng.py /usr/local/bin/enum4linux-ng</code></pre><p><strong>responder</strong> - LLMNR, NBT-NS and MDNS poisoner for credential harvesting</p><pre><code class=\"language-bash\">sudo apt install git python3-pip python3-aioquic python3-netifaces\ngit clone https://github.com/lgandx/Responder.git\ncd Responder\nsudo chmod +x Responder.py\n## Run from any directory\nsudo tee /usr/local/bin/responder &gt; /dev/null &lt;&lt;'EOF'\n#!/bin/bash\ncd /opt/Responder\npython3 Responder.py \"$@\"\nEOF\nsudo chmod +x /usr/local/bin/responder</code></pre><p><strong>metasploit-framework</strong> - Comprehensive penetration testing framework</p><pre><code class=\"language-bash\">curl https://raw.githubusercontent.com/rapid7/metasploit-omnibus/master/config/templates/metasploit-framework-wrappers/msfupdate.erb &gt; msfinstall\nchmod 755 msfinstall\n./msfinstall</code></pre><p><strong>bloodhound-python</strong> - Active Directory reconnaissance and attack path analysis</p><pre><code class=\"language-bash\">sudo apt install bloodhound.py</code></pre><p><strong>kerbrute</strong> - Tool for brute-forcing and enumerating Kerberos accounts</p><pre><code class=\"language-bash\"># Install from releases\nwget https://github.com/ropnop/kerbrute/releases/download/v1.0.3/kerbrute_linux_arm64\nchmod +x kerbrute_linux_arm64\nsudo mv kerbrute_linux_arm64 /usr/local/bin/kerbrute</code></pre><p><strong>ncat</strong> - Modernized netcat with SSL/TLS support</p><pre><code class=\"language-bash\">sudo apt install ncat</code></pre><p><strong>secator</strong> - Security automation and orchestration framework</p><pre><code class=\"language-bash\">pipx install secator</code></pre><h3 id=\"web-application-reconnaissance\">Web Application Reconnaissance</h3><p><strong>surf</strong> - Simple webkit-based web browser (useful for automated browsing)</p><pre><code class=\"language-bash\">sudo apt install surf</code></pre><p><strong>gowitness</strong> - Web screenshot and report generation tool</p><pre><code class=\"language-bash\">go install github.com/sensepost/gowitness@latest\nsudo cp ~/go/bin/gowitness /usr/local/bin/</code></pre><p><strong>aquatone</strong> - Domain flyover tool for visual inspection of websites</p><pre><code class=\"language-bash\">wget https://github.com/michenriksen/aquatone/releases/download/v1.7.0/aquatone_linux_amd64_1.7.0.zip\nunzip aquatone_linux_amd64_1.7.0.zip -d ~/Applications/aquatone\nsudo ln -s ~/Applications/aquatone/aquatone /usr/local/bin/aquatone\nrm aquatone_linux_amd64_1.7.0.zip</code></pre><p><strong>subjack</strong> - Subdomain takeover detection tool</p><pre><code class=\"language-bash\"># Install Go if not already installed\nsudo apt install golang-go\n\n# Install subjack via go install\ngo install github.com/haccer/subjack@latest\n\n# Move to system path\nsudo cp ~/go/bin/subjack /usr/local/bin/</code></pre><p><strong>httpx</strong> - Fast HTTP toolkit for probing web servers</p><pre><code class=\"language-bash\">go install -v github.com/projectdiscovery/httpx/cmd/httpx@latest\nsudo mv ~/go/bin/httpx /usr/local/bin/</code></pre><p><strong>katana</strong> - Next-generation crawling and spidering framework</p><pre><code class=\"language-bash\">go install github.com/projectdiscovery/katana/cmd/katana@latest\nsudo mv ~/go/bin/katana /usr/local/bin/</code></pre><p><strong>photon</strong> - Fast web crawler for OSINT</p><pre><code class=\"language-bash\">git clone https://github.com/s0md3v/Photon.git ~/Applications/Photon\ncd ~/Applications/Photon\npython3 -m venv venv\nsource venv/bin/activate\npip install -r requirements.txt\ndeactivate\n\n# Create wrapper script\nsudo tee /usr/local/bin/photon &gt; /dev/null &lt;&lt;'EOF'\n#!/bin/bash\ncd ~/Applications/Photon\nsource venv/bin/activate\npython photon.py \"$@\"\ndeactivate\nEOF\nsudo chmod +x /usr/local/bin/photon</code></pre><p><strong>waybackurls</strong> - Fetch URLs from Wayback Machine</p><pre><code class=\"language-bash\">go install github.com/tomnomnom/waybackurls@latest\nsudo mv ~/go/bin/waybackurls /usr/local/bin/</code></pre><p><strong>assetfinder</strong> - Find domains and subdomains related to a given domain</p><pre><code class=\"language-bash\"># Install via go\ngo install github.com/tomnomnom/assetfinder@latest\nsudo cp ~/go/bin/assetfinder /usr/local/bin/</code></pre><p><strong>gitdorker</strong> - GitHub dorking tool for finding sensitive information</p><pre><code class=\"language-bash\">git clone https://github.com/obheda12/GitDorker.git ~/Applications/GitDorker\ncd ~/Applications/GitDorker\npipx install -e .</code></pre><p><strong>emailharvester</strong> - Email harvesting tool for OSINT</p><pre><code class=\"language-bash\">git clone https://github.com/maldevel/EmailHarvester.git ~/Applications/EmailHarvester\ncd ~/Applications/EmailHarvester\npython3 -m venv venv\nsource venv/bin/activate\npip install -r requirements.txt\ndeactivate\n\n# Create wrapper script\nsudo tee /usr/local/bin/emailharvester &gt; /dev/null &lt;&lt;'EOF'\n#!/bin/bash\ncd ~/Applications/EmailHarvester\nsource venv/bin/activate\npython EmailHarvester.py \"$@\"\ndeactivate\nEOF\nsudo chmod +x /usr/local/bin/emailharvester</code></pre><h3 id=\"command-control-c2-frameworks\">Command &amp; Control (C2) Frameworks</h3><p><strong>sliver</strong> - Modern cross-platform adversary emulation/red team framework</p><pre><code class=\"language-bash\"># Install Go if not already installed\nsudo apt install golang-go\n\n# Set Go environment\necho 'export PATH=$PATH:/usr/local/go/bin:~/go/bin' &gt;&gt; ~/.bashrc\nsource ~/.bashrc\n\n# Clone and build Sliver\ncd /opt\nsudo git clone https://github.com/BishopFox/sliver.git\ncd sliver\nsudo make linux-arm64\n\n# The binaries will be created in the current directory\nsudo cp sliver-server sliver-client /usr/local/bin/\nsudo chmod +x /usr/local/bin/sliver-*</code></pre><p><strong>covenant</strong> - .NET C2 framework for red team operations</p><pre><code class=\"language-bash\"># Requires .NET SDK\nsudo apt install -y dotnet-sdk-8.0\ngit clone --recurse-submodules https://github.com/cobbr/Covenant.git ~/Applications/Covenant\n# Build manually: cd ~/Applications/Covenant/Covenant &amp;&amp; dotnet build</code></pre><h3 id=\"cloud-security-tools\">Cloud Security Tools</h3><p><strong>scoutsuite</strong> - Multi-cloud security auditing tool</p><pre><code class=\"language-bash\">pipx install scoutsuite</code></pre><p><strong>prowler</strong> - AWS/Azure/GCP security assessment tool</p><pre><code class=\"language-bash\">pipx install prowler</code></pre><p><strong>cloudmapper</strong> - AWS environment visualization and analysis</p><pre><code class=\"language-bash\">git clone https://github.com/duo-labs/cloudmapper.git ~/Applications/cloudmapper\ncd ~/Applications/cloudmapper\npipx install .</code></pre><h3 id=\"mobile-security-testing\">Mobile Security Testing</h3><p><strong>mobsf</strong> - Mobile Security Framework for automated mobile app security testing</p><pre><code class=\"language-bash\">git clone https://github.com/MobSF/Mobile-Security-Framework-MobSF.git ~/Applications/MobSF\ncd ~/Applications/MobSF\n./setup.sh\n# Run with: ./run.sh</code></pre><p><strong>frida-tools</strong> - Dynamic instrumentation toolkit for mobile apps</p><pre><code class=\"language-bash\">pipx install frida-tools</code></pre><p><strong>objection</strong> - Runtime mobile security assessment framework</p><pre><code class=\"language-bash\">pipx install objection</code></pre><h3 id=\"windowsactive-directory-tools\">Windows/Active Directory Tools</h3><p><strong>powersploit</strong> - PowerShell post-exploitation framework modules</p><pre><code class=\"language-bash\">git clone https://github.com/PowerShellMafia/PowerSploit.git ~/Applications/PowerSploit\n# Use modules as needed from this directory</code></pre><h3 id=\"password-username-generation\">Password &amp; Username Generation</h3><p><strong>cupp</strong> - Common User Passwords Profiler for targeted password list generation</p><pre><code class=\"language-bash\">sudo apt install cupp</code></pre><p><strong>username-anarchy</strong> - Generate username lists from names for enumeration</p><pre><code class=\"language-bash\">git clone https://github.com/urbanadventurer/username-anarchy.git ~/Applications/username-anarchy\nsudo ln -s ~/Applications/username-anarchy/username-anarchy /usr/local/bin/username-anarc</code></pre><h3 id=\"protocol-specific-service-enumeration\">Protocol-Specific &amp; Service Enumeration</h3><p><strong>smbclient</strong> - Access and test SMB/CIFS shares on remote servers</p><pre><code class=\"language-bash\">sudo apt install smbclient</code></pre><p><strong>smbmap</strong> - SMB share enumeration and access testing</p><pre><code class=\"language-bash\">sudo apt install smbmap</code></pre><p><strong>ldapsearch</strong> - Query and enumerate LDAP directory services</p><pre><code class=\"language-bash\">sudo apt install ldap-utils</code></pre><p><strong>snmpwalk</strong> - Walk through SNMP MIB trees on network devices</p><pre><code class=\"language-bash\">sudo apt install snmp</code></pre><p><strong>snmpget</strong> - Retrieve specific SNMP values from remote systems</p><pre><code class=\"language-bash\">sudo apt install snmp</code></pre><p><strong>rpcinfo</strong> - Query RPC services on remote systems</p><pre><code class=\"language-bash\">sudo apt install rpcbind</code></pre><p><strong>showmount</strong> - Display NFS exports on remote servers</p><pre><code class=\"language-bash\">sudo apt install nfs-common</code></pre><p><strong>redis-cli</strong> - Redis database client for testing and exploitation</p><pre><code class=\"language-bash\">sudo apt install redis-tools</code></pre><p><strong>mysql-client</strong> - Connect to and test remote MySQL/MariaDB servers</p><pre><code class=\"language-bash\">sudo apt install default-mysql-client</code></pre><p><strong>postgresql-client</strong> - Connect to and test remote PostgreSQL servers</p><pre><code class=\"language-bash\">sudo apt install postgresql-client</code></pre><h3 id=\"wireless-network-testing\">Wireless Network Testing</h3><p><strong>aircrack-ng</strong> - Complete WiFi security auditing and cracking suite</p><pre><code class=\"language-bash\">sudo apt install aircrack-ng</code></pre><p><strong>kismet</strong> - Wireless network detector, sniffer and intrusion detection</p><pre><code class=\"language-bash\">wget -O - https://www.kismetwireless.net/repos/kismet-release.gpg.key --quiet | gpg --dearmor | sudo tee /usr/share/keyrings/kismet-archive-keyring.gpg &gt;/dev/null\necho 'deb [signed-by=/usr/share/keyrings/kismet-archive-keyring.gpg] https://www.kismetwireless.net/repos/apt/release/trixie trixie main' | sudo tee /etc/apt/sources.list.d/kismet.list &gt;/dev/null\nsudo apt update\nsudo apt install kismet\nsudo usermod -aG kismet $USER</code></pre><p><strong>wifite</strong> - Automated wireless attack tool</p><pre><code class=\"language-bash\">sudo apt install wifite</code></pre><p><strong>wavemon</strong> - Ncurses-based wireless network monitoring tool</p><pre><code class=\"language-bash\">sudo apt install wavemon</code></pre><h3 id=\"miscellaneous-tools\">Miscellaneous Tools</h3><p><strong>btop</strong> - Modern resource monitor with beautiful interface</p><pre><code class=\"language-bash\">sudo apt install btop</code></pre><p><strong>notify</strong> - Send notifications from CLI (supports Slack, Discord, etc.)</p><p>bash</p><pre><code class=\"language-bash\">go install -v github.com/projectdiscovery/notify/cmd/notify@latest\nsudo mv ~/go/bin/notify /usr/local/bin/</code></pre><p><strong>curlie</strong> - Modern curl alternative with better syntax</p><p>bash</p><pre><code class=\"language-bash\">curl -sS https://webinstall.dev/curlie | bash\nsource ~/.config/envman/PATH.env</code></pre><p><strong>bat</strong> - Cat clone with syntax highlighting and Git integration</p><p>bash</p><pre><code class=\"language-bash\">sudo apt install bat</code></pre><h3 id=\"comprehensive-installation-command\">Comprehensive Installation Command</h3><p>Install most essential modern tools at once:</p><pre><code class=\"language-bash\">sudo apt update\nsudo apt install -y nmap masscan arp-scan fping hping3 snmp snmp-mibs-downloader \\\n  tcpdump tshark bettercap \\\n  iperf3 mtr \\\n  nikto sqlmap hydra ffuf gobuster python3-impacket ncat \\\n  smbclient smbmap ldap-utils rpcbind nfs-common redis-tools default-mysql-client postgresql-client \\\n  aircrack-ng wifite wavemon \\\n  pipx</code></pre><p>Then install tools not in apt repositories:</p><pre><code class=\"language-bash\"># NetExec (essential!)\n## Install Rust\ncurl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh\nsource \"$HOME/.cargo/env\"\nsudo apt install -y build-essential libssl-dev libffi-dev python3-dev pipx\npipx install git+https://github.com/Pennyw0rth/NetExec\n\n# RustScan (optional but very fast)\ncargo install rustscan\n\n# Kerbrute\nwget https://github.com/ropnop/kerbrute/releases/download/v1.0.3/kerbrute_linux_arm64\nchmod +x kerbrute_linux_arm64\nsudo mv kerbrute_linux_arm64 /usr/local/bin/kerbrute\n\n# Nuclei\nwget https://github.com/projectdiscovery/nuclei/releases/download/v3.3.6/nuclei_3.3.6_linux_arm64.zip\nunzip nuclei_3.3.6_linux_arm64.zip\nsudo mv nuclei /usr/local/bin/\nsudo chmod +x /usr/local/bin/nuclei\n\n# WPscan\nsudo apt install ruby ruby-dev libcurl4-openssl-dev make zlib1g-dev\nsudo gem install wpscan\n\n# Enum4linux\nsudo apt install git smbclient python3-ldap3 python3-yaml python3-impacket\ngit clone https://github.com/cddmp/enum4linux-ng.git\ncd enum4linux-ng\nsudo chmod +x enum4linux-ng.py\nsudo cp enum4linux-ng.py /usr/local/bin/enum4linux-ng\n\n# Responder\nsudo apt install git python3-pip python3-aioquic python3-netifaces\ngit clone https://github.com/lgandx/Responder.git\ncd Responder\nsudo chmod +x Responder.py\n## Run from any directory\nsudo tee /usr/local/bin/responder &gt; /dev/null &lt;&lt;'EOF'\n#!/bin/bash\ncd /opt/Responder\npython3 Responder.py \"$@\"\nEOF\nsudo chmod +x /usr/local/bin/responder\n\n# Kismet\nwget -O - https://www.kismetwireless.net/repos/kismet-release.gpg.key --quiet | gpg --dearmor | sudo tee /usr/share/keyrings/kismet-archive-keyring.gpg &gt;/dev/null\necho 'deb [signed-by=/usr/share/keyrings/kismet-archive-keyring.gpg] https://www.kismetwireless.net/repos/apt/release/trixie trixie main' | sudo tee /etc/apt/sources.list.d/kismet.list &gt;/dev/null\nsudo apt update\nsudo apt install kismet</code></pre>",
    "plaintext": "Initial Configuration\n\nThis is a Raspberry Pi 5 that I setup to have access to tools no matter which network I attach it to. The Raspberry Pi is the 8GB model and I used a PoE hat that included the SSD slot.\n\nThe first thing that I did was use Raspberry Pi Imager and image a nvme SSD with the Lite Raspberry Pi Linux. I added my SSH settings into the image during the flashing process. This ensured that once I had the IP address, I could connect remotely to it without every hooking up a monitor or keyboard.\n\nThe second thing was to install Tailscale. Any VPN solution would work, but I have been using Tailscale for my home lab for a while now and like how easy it is to configure. After installing Tailscale, the following commands allowed me to connect and expose SSH to my Tailnet (Tailscale network).\n\nsudo tailscale up --ssh\nsudo systemctl enable tailscaled\nsudo tailscale set --operator=$USER\n\nThe first command connects and exposes SSH. The second command sets the Tailscale service (tailscaled) to start at boot. The third command is to allow the current user SSH access. The default setup for Tailscale is only to allow root SSH access. Now once I plug the Raspberry Pi into a network and it pulls a DHCP address, I will have a remote connection to it over my Tailnet. As long as Tailscale would work on the network, I will have SSH access to it.\n\nThen I add some tools. I could just flash a Kali image onto the Raspberry Pi, but I don't want the operating system to be detected as Kali. The first reason is because I will use this box as a troubleshooting tool and I don't want it be be identified as an intrusion. The second reason is the same, but I don't want it to be fingerprinted if I am doing offensive work.\n\n\nTools\n\n\nPrerequisites\n\n * Go\n\nsudo apt install golang-go\necho 'export PATH=$PATH:~/go/bin' >> ~/.bashrc\nsource ~/.bashrc\n\n\nNetwork Scanning & Discovery\n\nnmap - Industry-standard network scanner for host discovery, port scanning, OS detection, and service enumeration\n\nsudo apt install nmap\n\nrustscan - Blazingly fast modern port scanner that feeds results to nmap\n\n# Install Rust if you haven't already\ncurl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh\nsource \"$HOME/.cargo/env\"\n\n# Install RustScan via cargo\ncargo install rustscan\n\nmasscan - Ultra-fast port scanner for large-scale network scanning\n\nsudo apt install masscan\n\narp-scan - Fast ARP-based local network device discovery\n\nsudo apt install arp-scan\n\nfping - Parallel ICMP echo probe tool for fast host checking\n\nsudo apt install fping\n\nhping3 - TCP/IP packet crafting and network testing tool\n\nsudo apt install hping3\n\nsnmp / snmpwalk - Query remote systems via SNMP protocol\n\nsudo apt install snmp snmp-mibs-downloader\n\n\nNetwork Traffic Analysis & Monitoring\n\ntcpdump - Powerful CLI packet capture and analysis tool\n\nsudo apt install tcpdump\n\ntshark - Wireshark's CLI for detailed packet analysis with filtering\n\nsudo apt install tshark\n\nbettercap - Modern network attack and monitoring framework\n\nsudo apt install bettercap\n\n\nNetwork Performance Testing\n\niperf3 - Standard tool for measuring TCP/UDP bandwidth between hosts\n\nsudo apt install iperf3\n\nmtr - Combined ping and traceroute with real-time statistics\n\nsudo apt install mtr\n\n\nSecurity Testing & Vulnerability Scanning\n\nnuclei - Modern, fast vulnerability scanner with extensive template library\n\nwget https://github.com/projectdiscovery/nuclei/releases/download/v3.3.6/nuclei_3.3.6_linux_arm64.zip\nunzip nuclei_3.3.6_linux_arm64.zip\nsudo mv nuclei /usr/local/bin/\nsudo chmod +x /usr/local/bin/nuclei\n\nnikto - Classic web server vulnerability scanner (older but still useful)\n\nsudo apt install nikto\n\nwpscan - WordPress security scanner and vulnerability database\n\nsudo apt install ruby ruby-dev libcurl4-openssl-dev make zlib1g-dev\nsudo gem install wpscan\n\nsqlmap - Automatic SQL injection detection and exploitation tool\n\nsudo apt install sqlmap\n\nhydra - Fast network login brute-forcer supporting many protocols\n\nsudo apt install hydra\n\nffuf - Modern, fast web fuzzer for directory/file discovery and fuzzing\n\nsudo apt install ffuf\n\nferoxbuster - Fast, recursive content discovery tool written in Rust\n\n# Install from releases (not in standard apt)\nwget https://github.com/epi052/feroxbuster/releases/download/v2.10.4/feroxbuster_arm64.deb.zip\nunzip feroxbuster_arm64.deb.zip\nsudo dpkg -i feroxbuster_*_arm64.deb\n\ngobuster - Fast directory/file & DNS enumeration tool\n\nsudo apt install gobuster\n\nnetexec - Swiss army knife for pentesting networks (replaces CrackMapExec)\n\n# Install Rust\ncurl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh\n\n# When prompted, choose option 1 (default installation)\n# Then activate Rust in current shell\nsource \"$HOME/.cargo/env\"\n\n\nsudo apt install -y build-essential libssl-dev libffi-dev python3-dev pipx\npipx install git+https://github.com/Pennyw0rth/NetExec\n\nimpacket-scripts - Essential collection of Python scripts for network protocols\n\nsudo apt install python3-impacket\n\nenum4linux-ng - Modern Python rewrite of enum4linux for Windows/Samba enumeration\n\nsudo apt install git smbclient python3-ldap3 python3-yaml python3-impacket\ngit clone https://github.com/cddmp/enum4linux-ng.git\ncd enum4linux-ng\nsudo chmod +x enum4linux-ng.py\nsudo cp enum4linux-ng.py /usr/local/bin/enum4linux-ng\n\nresponder - LLMNR, NBT-NS and MDNS poisoner for credential harvesting\n\nsudo apt install git python3-pip python3-aioquic python3-netifaces\ngit clone https://github.com/lgandx/Responder.git\ncd Responder\nsudo chmod +x Responder.py\n## Run from any directory\nsudo tee /usr/local/bin/responder > /dev/null <<'EOF'\n#!/bin/bash\ncd /opt/Responder\npython3 Responder.py \"$@\"\nEOF\nsudo chmod +x /usr/local/bin/responder\n\nmetasploit-framework - Comprehensive penetration testing framework\n\ncurl https://raw.githubusercontent.com/rapid7/metasploit-omnibus/master/config/templates/metasploit-framework-wrappers/msfupdate.erb > msfinstall\nchmod 755 msfinstall\n./msfinstall\n\nbloodhound-python - Active Directory reconnaissance and attack path analysis\n\nsudo apt install bloodhound.py\n\nkerbrute - Tool for brute-forcing and enumerating Kerberos accounts\n\n# Install from releases\nwget https://github.com/ropnop/kerbrute/releases/download/v1.0.3/kerbrute_linux_arm64\nchmod +x kerbrute_linux_arm64\nsudo mv kerbrute_linux_arm64 /usr/local/bin/kerbrute\n\nncat - Modernized netcat with SSL/TLS support\n\nsudo apt install ncat\n\nsecator - Security automation and orchestration framework\n\npipx install secator\n\n\nWeb Application Reconnaissance\n\nsurf - Simple webkit-based web browser (useful for automated browsing)\n\nsudo apt install surf\n\ngowitness - Web screenshot and report generation tool\n\ngo install github.com/sensepost/gowitness@latest\nsudo cp ~/go/bin/gowitness /usr/local/bin/\n\naquatone - Domain flyover tool for visual inspection of websites\n\nwget https://github.com/michenriksen/aquatone/releases/download/v1.7.0/aquatone_linux_amd64_1.7.0.zip\nunzip aquatone_linux_amd64_1.7.0.zip -d ~/Applications/aquatone\nsudo ln -s ~/Applications/aquatone/aquatone /usr/local/bin/aquatone\nrm aquatone_linux_amd64_1.7.0.zip\n\nsubjack - Subdomain takeover detection tool\n\n# Install Go if not already installed\nsudo apt install golang-go\n\n# Install subjack via go install\ngo install github.com/haccer/subjack@latest\n\n# Move to system path\nsudo cp ~/go/bin/subjack /usr/local/bin/\n\nhttpx - Fast HTTP toolkit for probing web servers\n\ngo install -v github.com/projectdiscovery/httpx/cmd/httpx@latest\nsudo mv ~/go/bin/httpx /usr/local/bin/\n\nkatana - Next-generation crawling and spidering framework\n\ngo install github.com/projectdiscovery/katana/cmd/katana@latest\nsudo mv ~/go/bin/katana /usr/local/bin/\n\nphoton - Fast web crawler for OSINT\n\ngit clone https://github.com/s0md3v/Photon.git ~/Applications/Photon\ncd ~/Applications/Photon\npython3 -m venv venv\nsource venv/bin/activate\npip install -r requirements.txt\ndeactivate\n\n# Create wrapper script\nsudo tee /usr/local/bin/photon > /dev/null <<'EOF'\n#!/bin/bash\ncd ~/Applications/Photon\nsource venv/bin/activate\npython photon.py \"$@\"\ndeactivate\nEOF\nsudo chmod +x /usr/local/bin/photon\n\nwaybackurls - Fetch URLs from Wayback Machine\n\ngo install github.com/tomnomnom/waybackurls@latest\nsudo mv ~/go/bin/waybackurls /usr/local/bin/\n\nassetfinder - Find domains and subdomains related to a given domain\n\n# Install via go\ngo install github.com/tomnomnom/assetfinder@latest\nsudo cp ~/go/bin/assetfinder /usr/local/bin/\n\ngitdorker - GitHub dorking tool for finding sensitive information\n\ngit clone https://github.com/obheda12/GitDorker.git ~/Applications/GitDorker\ncd ~/Applications/GitDorker\npipx install -e .\n\nemailharvester - Email harvesting tool for OSINT\n\ngit clone https://github.com/maldevel/EmailHarvester.git ~/Applications/EmailHarvester\ncd ~/Applications/EmailHarvester\npython3 -m venv venv\nsource venv/bin/activate\npip install -r requirements.txt\ndeactivate\n\n# Create wrapper script\nsudo tee /usr/local/bin/emailharvester > /dev/null <<'EOF'\n#!/bin/bash\ncd ~/Applications/EmailHarvester\nsource venv/bin/activate\npython EmailHarvester.py \"$@\"\ndeactivate\nEOF\nsudo chmod +x /usr/local/bin/emailharvester\n\n\nCommand & Control (C2) Frameworks\n\nsliver - Modern cross-platform adversary emulation/red team framework\n\n# Install Go if not already installed\nsudo apt install golang-go\n\n# Set Go environment\necho 'export PATH=$PATH:/usr/local/go/bin:~/go/bin' >> ~/.bashrc\nsource ~/.bashrc\n\n# Clone and build Sliver\ncd /opt\nsudo git clone https://github.com/BishopFox/sliver.git\ncd sliver\nsudo make linux-arm64\n\n# The binaries will be created in the current directory\nsudo cp sliver-server sliver-client /usr/local/bin/\nsudo chmod +x /usr/local/bin/sliver-*\n\ncovenant - .NET C2 framework for red team operations\n\n# Requires .NET SDK\nsudo apt install -y dotnet-sdk-8.0\ngit clone --recurse-submodules https://github.com/cobbr/Covenant.git ~/Applications/Covenant\n# Build manually: cd ~/Applications/Covenant/Covenant && dotnet build\n\n\nCloud Security Tools\n\nscoutsuite - Multi-cloud security auditing tool\n\npipx install scoutsuite\n\nprowler - AWS/Azure/GCP security assessment tool\n\npipx install prowler\n\ncloudmapper - AWS environment visualization and analysis\n\ngit clone https://github.com/duo-labs/cloudmapper.git ~/Applications/cloudmapper\ncd ~/Applications/cloudmapper\npipx install .\n\n\nMobile Security Testing\n\nmobsf - Mobile Security Framework for automated mobile app security testing\n\ngit clone https://github.com/MobSF/Mobile-Security-Framework-MobSF.git ~/Applications/MobSF\ncd ~/Applications/MobSF\n./setup.sh\n# Run with: ./run.sh\n\nfrida-tools - Dynamic instrumentation toolkit for mobile apps\n\npipx install frida-tools\n\nobjection - Runtime mobile security assessment framework\n\npipx install objection\n\n\nWindows/Active Directory Tools\n\npowersploit - PowerShell post-exploitation framework modules\n\ngit clone https://github.com/PowerShellMafia/PowerSploit.git ~/Applications/PowerSploit\n# Use modules as needed from this directory\n\n\nPassword & Username Generation\n\ncupp - Common User Passwords Profiler for targeted password list generation\n\nsudo apt install cupp\n\nusername-anarchy - Generate username lists from names for enumeration\n\ngit clone https://github.com/urbanadventurer/username-anarchy.git ~/Applications/username-anarchy\nsudo ln -s ~/Applications/username-anarchy/username-anarchy /usr/local/bin/username-anarc\n\n\nProtocol-Specific & Service Enumeration\n\nsmbclient - Access and test SMB/CIFS shares on remote servers\n\nsudo apt install smbclient\n\nsmbmap - SMB share enumeration and access testing\n\nsudo apt install smbmap\n\nldapsearch - Query and enumerate LDAP directory services\n\nsudo apt install ldap-utils\n\nsnmpwalk - Walk through SNMP MIB trees on network devices\n\nsudo apt install snmp\n\nsnmpget - Retrieve specific SNMP values from remote systems\n\nsudo apt install snmp\n\nrpcinfo - Query RPC services on remote systems\n\nsudo apt install rpcbind\n\nshowmount - Display NFS exports on remote servers\n\nsudo apt install nfs-common\n\nredis-cli - Redis database client for testing and exploitation\n\nsudo apt install redis-tools\n\nmysql-client - Connect to and test remote MySQL/MariaDB servers\n\nsudo apt install default-mysql-client\n\npostgresql-client - Connect to and test remote PostgreSQL servers\n\nsudo apt install postgresql-client\n\n\nWireless Network Testing\n\naircrack-ng - Complete WiFi security auditing and cracking suite\n\nsudo apt install aircrack-ng\n\nkismet - Wireless network detector, sniffer and intrusion detection\n\nwget -O - https://www.kismetwireless.net/repos/kismet-release.gpg.key --quiet | gpg --dearmor | sudo tee /usr/share/keyrings/kismet-archive-keyring.gpg >/dev/null\necho 'deb [signed-by=/usr/share/keyrings/kismet-archive-keyring.gpg] https://www.kismetwireless.net/repos/apt/release/trixie trixie main' | sudo tee /etc/apt/sources.list.d/kismet.list >/dev/null\nsudo apt update\nsudo apt install kismet\nsudo usermod -aG kismet $USER\n\nwifite - Automated wireless attack tool\n\nsudo apt install wifite\n\nwavemon - Ncurses-based wireless network monitoring tool\n\nsudo apt install wavemon\n\n\nMiscellaneous Tools\n\nbtop - Modern resource monitor with beautiful interface\n\nsudo apt install btop\n\nnotify - Send notifications from CLI (supports Slack, Discord, etc.)\n\nbash\n\ngo install -v github.com/projectdiscovery/notify/cmd/notify@latest\nsudo mv ~/go/bin/notify /usr/local/bin/\n\ncurlie - Modern curl alternative with better syntax\n\nbash\n\ncurl -sS https://webinstall.dev/curlie | bash\nsource ~/.config/envman/PATH.env\n\nbat - Cat clone with syntax highlighting and Git integration\n\nbash\n\nsudo apt install bat\n\n\nComprehensive Installation Command\n\nInstall most essential modern tools at once:\n\nsudo apt update\nsudo apt install -y nmap masscan arp-scan fping hping3 snmp snmp-mibs-downloader \\\n  tcpdump tshark bettercap \\\n  iperf3 mtr \\\n  nikto sqlmap hydra ffuf gobuster python3-impacket ncat \\\n  smbclient smbmap ldap-utils rpcbind nfs-common redis-tools default-mysql-client postgresql-client \\\n  aircrack-ng wifite wavemon \\\n  pipx\n\nThen install tools not in apt repositories:\n\n# NetExec (essential!)\n## Install Rust\ncurl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh\nsource \"$HOME/.cargo/env\"\nsudo apt install -y build-essential libssl-dev libffi-dev python3-dev pipx\npipx install git+https://github.com/Pennyw0rth/NetExec\n\n# RustScan (optional but very fast)\ncargo install rustscan\n\n# Kerbrute\nwget https://github.com/ropnop/kerbrute/releases/download/v1.0.3/kerbrute_linux_arm64\nchmod +x kerbrute_linux_arm64\nsudo mv kerbrute_linux_arm64 /usr/local/bin/kerbrute\n\n# Nuclei\nwget https://github.com/projectdiscovery/nuclei/releases/download/v3.3.6/nuclei_3.3.6_linux_arm64.zip\nunzip nuclei_3.3.6_linux_arm64.zip\nsudo mv nuclei /usr/local/bin/\nsudo chmod +x /usr/local/bin/nuclei\n\n# WPscan\nsudo apt install ruby ruby-dev libcurl4-openssl-dev make zlib1g-dev\nsudo gem install wpscan\n\n# Enum4linux\nsudo apt install git smbclient python3-ldap3 python3-yaml python3-impacket\ngit clone https://github.com/cddmp/enum4linux-ng.git\ncd enum4linux-ng\nsudo chmod +x enum4linux-ng.py\nsudo cp enum4linux-ng.py /usr/local/bin/enum4linux-ng\n\n# Responder\nsudo apt install git python3-pip python3-aioquic python3-netifaces\ngit clone https://github.com/lgandx/Responder.git\ncd Responder\nsudo chmod +x Responder.py\n## Run from any directory\nsudo tee /usr/local/bin/responder > /dev/null <<'EOF'\n#!/bin/bash\ncd /opt/Responder\npython3 Responder.py \"$@\"\nEOF\nsudo chmod +x /usr/local/bin/responder\n\n# Kismet\nwget -O - https://www.kismetwireless.net/repos/kismet-release.gpg.key --quiet | gpg --dearmor | sudo tee /usr/share/keyrings/kismet-archive-keyring.gpg >/dev/null\necho 'deb [signed-by=/usr/share/keyrings/kismet-archive-keyring.gpg] https://www.kismetwireless.net/repos/apt/release/trixie trixie main' | sudo tee /etc/apt/sources.list.d/kismet.list >/dev/null\nsudo apt update\nsudo apt install kismet",
    "feature_image": null,
    "excerpt": "Initial Configuration\n\nThis is a Raspberry Pi 5 that I setup to have access to tools no matter which network I attach it to. The Raspberry Pi is the 8GB model and I used a PoE hat that included the SS...",
    "published_at": "2025-10-18T22:10:31.000Z",
    "published_date": "October 18, 2025",
    "tags": [
      "Home-Lab"
    ],
    "featured": false
  },
  {
    "id": "certification-sprint",
    "title": "Certification Sprint",
    "slug": "certification-sprint",
    "html": "<p>At the end of 2024, I took the beta test for the CompTIA Pentest+.  I was not really prepared, but I thought I would give it a try anyways.  This exam was the first cyber security certification test that I failed.  Even though I wasn't overly confident going into the test, I still thought I could pass.  </p><p>After failing this exam, I doubled down on studying offensive techniques.  I started doing modules in the Hack the Box Academy.  These helped me to get an understanding of tools, but most of the modules walked you through an attack and the asked you to perform the exact same attack.  If you were unsure of what to do, you could just scroll up and there the answer was.  Not all of the modules were like this though.  There were a few that altered the box enough that it was a completely different experience.</p><p>I also setup a Kali VM and a few vulnerable machines, including Metasploitable 2 and 3, in Proxmox.  I liked using Proxmox for my cyber lab because I could take snapshots of the machines and easily revert back to the beginning state.  The snapshots were also stored on a separate machine running Proxmox Backup Server, so even when my hardware eventually failed, it only took me around 15 minutes to get completely up and running again.</p><p> One day I got an email from my councilor at Champlain College.  She said that I would have enough credits to graduate the following Spring or Summer depending on whether or not I took a couple extra classes one semester.  This was kind of a surprise to me.  I knew that I was doing good and that certifications had put me a little bit ahead, but I was getting reading to graduate after just three years.  This was both a good and bad thing.  I was using my student discount to make my certifications more affordable since I was paying for them out of my own pocket.</p><p>Realizing that my discount would end soon, I went ahead and bought another attempt at the Pentest+.  I was going on vacation with my family in July so I scheduled the test for the day before we were going to leave.  I thought that it would help to stop me from thinking about it over vacation and if I passed I would have an extra reason to be happy on vacation.  I went into the exam confident and passed.  After two attempts, I had my Pentest+.  </p><p>We ended up spending less money on vacation than I thought and when we got back I bought the exam and a retake attempt for the Cysa+ and the SecurityX exams.  I had the retake so I thought I should go ahead and get a feel for the Cysa+.  One week after returning from vacation, I took the test and passed it as well.  </p><p>Now I was on a roll and feeling great.  I thought I would go all in and give the SecurityX a try.  I scheduled it for two weeks later.  I studied hard, but thought I was not prepared enough.  I went into the exam nervous about failing even though I had a retake attempt.  I gave it my all and then waited afterwards.  The SecurityX was different from the other exams because it was not graded.  It was either pass or fail.  There was no score.  The results were also not immediate.  I waited a few hours and then the email came in.  I passed it. </p><p>I was over the moon now.  When I started school, my goal was to get a degree and the Security+ certification.  Now I was in my last year of school and had every CompTIA cyber security certification offered.  I was amazed and inspired, but also had to figure out what came next.  I had the desire to learn more and wanted a way to prove my knowledge if I could not get a cyber security job right away.</p><p>This is when I came across TCM Security.  They offered certifications that were comparable to Hack the Box or Offsec exams, but more affordable.  They also had a discount running at this time.  So, I bought the TCM Practical Web Penetration Associate Exam.  I had already been studying most of these topics, so I gave myself three weeks to prepare and then took this exam. </p><p>It was different from the CompTIA exams in that it was a hands on exam and there were no questions at all.  The goal was to compromise a web app and then write a professional report.  The exam lasted for three days.  The first day, I managed to find three critical vulnerabilities and several medium vulnerabilities.  I wrote the report the second and and submitted it.  It seemed like it took forever, but three days later I got the results.  I passed it as well.</p><p>I didn't set out to sprint through certifications, but that is what happened.  From July 13 to August 25, I had taken and passed four different certification exams.  Before taking these exams, I had never heard of stackable certifications, but now I had as many stackable CompTIA certifications as I did standalone ones.</p><p>After all this, I had several certifications, was almost through my degree, and had over a year experience in IT.  It was an amazing feeling to actually be forcing change in my life.  I would recommend anyone that is thinking about changing their life to go for it.  The feeling of regret is so much worse than the feeling of failing.  I would spend a few days, maybe a month, thinking about a failure, but I spent years dwelling on regret.</p>",
    "plaintext": "At the end of 2024, I took the beta test for the CompTIA Pentest+. I was not really prepared, but I thought I would give it a try anyways. This exam was the first cyber security certification test that I failed. Even though I wasn't overly confident going into the test, I still thought I could pass.\n\nAfter failing this exam, I doubled down on studying offensive techniques. I started doing modules in the Hack the Box Academy. These helped me to get an understanding of tools, but most of the modules walked you through an attack and the asked you to perform the exact same attack. If you were unsure of what to do, you could just scroll up and there the answer was. Not all of the modules were like this though. There were a few that altered the box enough that it was a completely different experience.\n\nI also setup a Kali VM and a few vulnerable machines, including Metasploitable 2 and 3, in Proxmox. I liked using Proxmox for my cyber lab because I could take snapshots of the machines and easily revert back to the beginning state. The snapshots were also stored on a separate machine running Proxmox Backup Server, so even when my hardware eventually failed, it only took me around 15 minutes to get completely up and running again.\n\nOne day I got an email from my councilor at Champlain College. She said that I would have enough credits to graduate the following Spring or Summer depending on whether or not I took a couple extra classes one semester. This was kind of a surprise to me. I knew that I was doing good and that certifications had put me a little bit ahead, but I was getting reading to graduate after just three years. This was both a good and bad thing. I was using my student discount to make my certifications more affordable since I was paying for them out of my own pocket.\n\nRealizing that my discount would end soon, I went ahead and bought another attempt at the Pentest+. I was going on vacation with my family in July so I scheduled the test for the day before we were going to leave. I thought that it would help to stop me from thinking about it over vacation and if I passed I would have an extra reason to be happy on vacation. I went into the exam confident and passed. After two attempts, I had my Pentest+.\n\nWe ended up spending less money on vacation than I thought and when we got back I bought the exam and a retake attempt for the Cysa+ and the SecurityX exams. I had the retake so I thought I should go ahead and get a feel for the Cysa+. One week after returning from vacation, I took the test and passed it as well.\n\nNow I was on a roll and feeling great. I thought I would go all in and give the SecurityX a try. I scheduled it for two weeks later. I studied hard, but thought I was not prepared enough. I went into the exam nervous about failing even though I had a retake attempt. I gave it my all and then waited afterwards. The SecurityX was different from the other exams because it was not graded. It was either pass or fail. There was no score. The results were also not immediate. I waited a few hours and then the email came in. I passed it.\n\nI was over the moon now. When I started school, my goal was to get a degree and the Security+ certification. Now I was in my last year of school and had every CompTIA cyber security certification offered. I was amazed and inspired, but also had to figure out what came next. I had the desire to learn more and wanted a way to prove my knowledge if I could not get a cyber security job right away.\n\nThis is when I came across TCM Security. They offered certifications that were comparable to Hack the Box or Offsec exams, but more affordable. They also had a discount running at this time. So, I bought the TCM Practical Web Penetration Associate Exam. I had already been studying most of these topics, so I gave myself three weeks to prepare and then took this exam.\n\nIt was different from the CompTIA exams in that it was a hands on exam and there were no questions at all. The goal was to compromise a web app and then write a professional report. The exam lasted for three days. The first day, I managed to find three critical vulnerabilities and several medium vulnerabilities. I wrote the report the second and and submitted it. It seemed like it took forever, but three days later I got the results. I passed it as well.\n\nI didn't set out to sprint through certifications, but that is what happened. From July 13 to August 25, I had taken and passed four different certification exams. Before taking these exams, I had never heard of stackable certifications, but now I had as many stackable CompTIA certifications as I did standalone ones.\n\nAfter all this, I had several certifications, was almost through my degree, and had over a year experience in IT. It was an amazing feeling to actually be forcing change in my life. I would recommend anyone that is thinking about changing their life to go for it. The feeling of regret is so much worse than the feeling of failing. I would spend a few days, maybe a month, thinking about a failure, but I spent years dwelling on regret.",
    "feature_image": null,
    "excerpt": "At the end of 2024, I took the beta test for the CompTIA Pentest+. I was not really prepared, but I thought I would give it a try anyways. This exam was the first cyber security certification test tha...",
    "published_at": "2025-10-17T21:58:17.000Z",
    "published_date": "October 17, 2025",
    "tags": [
      "Personal Growth"
    ],
    "featured": false
  },
  {
    "id": "first-it-job",
    "title": "First IT Job",
    "slug": "first-it-job",
    "html": "<p>After close to a year of applying for IT jobs with no success, I finally landed a role at a local non-profit, RBR Alliance.  I was hired on in April 2024 as an IT assistant.  My duties were basically to handle any tasks that my boss tasked me with.</p><p>RBR Alliance is a non-profit that performs administration duties for three other non-profits, Blue River Services, Rauch Incorporated, and First Chance Center.  Between all four companies there are close to 1000 employees and a total of three IT roles.  When I was hired, only the IT director role was filled.  </p><p>I did not expect to learn a whole lot at this job.  I figured I might be helping to fix problems in Word or Excel or troubleshooting email issues.  I was wrong about that though.  I was surprised at the technological depth of the agency.  We managed three Microsoft 365 tenants, three on-premise Active Directory domains, 10 servers at three different locations, and anything thing else that was IT related.  </p><p>I quickly learned how to create and configure accounts in M365 and Active Directory.  I learned how to change and apply Group Policy to ensure employees only have the permissions needed for their role.  My first week I installed and configured Security Onion at a secondary location.</p><p>In the first month, I learned how to terminate and test ethernet cables, mostly unshielded Cat5, but also some shielded cable.  There were some tools that I was loaned at work, but in the spirit of investing in myself and because I always like to have my own tools, I bought the tools required to test, trace, and terminate cable.  At first, it took me close to ten minutes to terminate a cable, but after a day of running and terminating, I could do it in less than three minutes.</p><p>I learned how to setup and configure Cisco ASAs and how to document and manage switches and patch panels.  I was motivated and thrived in unfamiliar environments, so it did not take me long to gain my boss's confidence.  I went from completing tasks that he assigned me to completing most tasks that were recieved by the help desk.  This allowed him to focus on designing bigger projects like switching the entire company to IP phones.</p>",
    "plaintext": "After close to a year of applying for IT jobs with no success, I finally landed a role at a local non-profit, RBR Alliance. I was hired on in April 2024 as an IT assistant. My duties were basically to handle any tasks that my boss tasked me with.\n\nRBR Alliance is a non-profit that performs administration duties for three other non-profits, Blue River Services, Rauch Incorporated, and First Chance Center. Between all four companies there are close to 1000 employees and a total of three IT roles. When I was hired, only the IT director role was filled.\n\nI did not expect to learn a whole lot at this job. I figured I might be helping to fix problems in Word or Excel or troubleshooting email issues. I was wrong about that though. I was surprised at the technological depth of the agency. We managed three Microsoft 365 tenants, three on-premise Active Directory domains, 10 servers at three different locations, and anything thing else that was IT related.\n\nI quickly learned how to create and configure accounts in M365 and Active Directory. I learned how to change and apply Group Policy to ensure employees only have the permissions needed for their role. My first week I installed and configured Security Onion at a secondary location.\n\nIn the first month, I learned how to terminate and test ethernet cables, mostly unshielded Cat5, but also some shielded cable. There were some tools that I was loaned at work, but in the spirit of investing in myself and because I always like to have my own tools, I bought the tools required to test, trace, and terminate cable. At first, it took me close to ten minutes to terminate a cable, but after a day of running and terminating, I could do it in less than three minutes.\n\nI learned how to setup and configure Cisco ASAs and how to document and manage switches and patch panels. I was motivated and thrived in unfamiliar environments, so it did not take me long to gain my boss's confidence. I went from completing tasks that he assigned me to completing most tasks that were recieved by the help desk. This allowed him to focus on designing bigger projects like switching the entire company to IP phones.",
    "feature_image": null,
    "excerpt": "After close to a year of applying for IT jobs with no success, I finally landed a role at a local non-profit, RBR Alliance. I was hired on in April 2024 as an IT assistant. My duties were basically to...",
    "published_at": "2025-10-16T18:27:58.000Z",
    "published_date": "October 16, 2025",
    "tags": [
      "Personal Growth"
    ],
    "featured": false
  },
  {
    "id": "first-frustration",
    "title": "First Frustration",
    "slug": "first-frustration",
    "html": "<p>I had just obtained the Network+ and Security+ from CompTIA and I thought that I would at least be able to obtain an entry-level job at local companies.  I was in for a rude awakening though.</p><p>At the time, I was only vaguely familiar with Linux and had a passing knowledge of Windows and Active Directory.  I had a lab setup and was working in it at least an hour or two a night.  I was enrolled at Champlain College and also had certifications from Microsoft, Cisco, and ISC2.  I thought that even though I did not have any professional experience, I would be able to find some kind of IT opportunity.</p><p>Then I hit the a wall.  I applied to at least 5 jobs every week.  I was motivated and knew that I had to succeed no matter what.  Most of my applications did not receive any form of reply and half were met with the generic \"We are not going to proceed with you at this time.\"  The employers that I did get the first phone interview with ended the hiring process when they realized I had no professional experience, which was on my resume.  </p><p>I could have kept my job until I graduated and then started to look for a job, but I needed an entry-level IT job right away.  My plan was to work a help desk style job while I was in school and then when I graduated I would have the certifications, degree, and experience.  If I didn't have three children at this time, I might have given up here and stuck with my dead-end job that allowed me to just barely get by.  How could I let my children see me give up, though?  I had made a plan and wanted to show them that anything is possible if you are willing to sacrifice for it.</p><p>I started applying for any job that was related to IT.  I spent almost as much time during this period working on my resume as I did studying.  I was determined to take my frustration from this experience and let it fuel me.  From that point on, every interview I had I always made sure to ask for some form of feedback or advice.  I printed a copy of the Michael Jordan quote \"I've missed more than 9,000 shots in my career. I've lost almost 300 games. Twenty-six times I've been trusted to take the game-winning shot and missed. I've failed over and over and over again in my life. And that is why I succeed.\" and taped it to the inside of my bedroom door.  It was my only real motivation on some days.</p><p>These failures were my missed shots and I knew that eventually I would sink that buzzer beater.</p>",
    "plaintext": "I had just obtained the Network+ and Security+ from CompTIA and I thought that I would at least be able to obtain an entry-level job at local companies. I was in for a rude awakening though.\n\nAt the time, I was only vaguely familiar with Linux and had a passing knowledge of Windows and Active Directory. I had a lab setup and was working in it at least an hour or two a night. I was enrolled at Champlain College and also had certifications from Microsoft, Cisco, and ISC2. I thought that even though I did not have any professional experience, I would be able to find some kind of IT opportunity.\n\nThen I hit the a wall. I applied to at least 5 jobs every week. I was motivated and knew that I had to succeed no matter what. Most of my applications did not receive any form of reply and half were met with the generic \"We are not going to proceed with you at this time.\" The employers that I did get the first phone interview with ended the hiring process when they realized I had no professional experience, which was on my resume.\n\nI could have kept my job until I graduated and then started to look for a job, but I needed an entry-level IT job right away. My plan was to work a help desk style job while I was in school and then when I graduated I would have the certifications, degree, and experience. If I didn't have three children at this time, I might have given up here and stuck with my dead-end job that allowed me to just barely get by. How could I let my children see me give up, though? I had made a plan and wanted to show them that anything is possible if you are willing to sacrifice for it.\n\nI started applying for any job that was related to IT. I spent almost as much time during this period working on my resume as I did studying. I was determined to take my frustration from this experience and let it fuel me. From that point on, every interview I had I always made sure to ask for some form of feedback or advice. I printed a copy of the Michael Jordan quote \"I've missed more than 9,000 shots in my career. I've lost almost 300 games. Twenty-six times I've been trusted to take the game-winning shot and missed. I've failed over and over and over again in my life. And that is why I succeed.\" and taped it to the inside of my bedroom door. It was my only real motivation on some days.\n\nThese failures were my missed shots and I knew that eventually I would sink that buzzer beater.",
    "feature_image": null,
    "excerpt": "I had just obtained the Network+ and Security+ from CompTIA and I thought that I would at least be able to obtain an entry-level job at local companies. I was in for a rude awakening though.\n\nAt the t...",
    "published_at": "2025-10-16T17:40:30.000Z",
    "published_date": "October 16, 2025",
    "tags": [
      "Personal Growth"
    ],
    "featured": false
  },
  {
    "id": "first-comptia-certifications",
    "title": "First CompTIA Certifications",
    "slug": "first-comptia-certifications",
    "html": "<p>In June, I used my student email address to get a discount on the Network+ certification.  I gave myself one month to study and prepare for the exam, but when it came time for my to take it, I was not overly confident.  I had read the study guides a few time by this point and set up my own practice network at home.</p><p>The first week of July, I took the Network+ test and passed it.  I was so excited that I purchased the Security+ voucher and scheduled the exam for 10 days later.  I wanted to build upon my success and was feeling confident.  I took and passed the exam.  At this point, I knew this was the career path that I wanted to take. </p><p>I doubled down on investing in a cyber lab.  I bought a used Sophos firewall and three mini pcs off eBay.  Two of the mini pcs had ram that was soldered on the board, but the third did not, so I invested in 64 GB or RAM.  This is when I discovered Pfsense and Proxmox.  Then I deployed several Linux server and an Active Directory domain.  I think this was the point in my journey that I became addicted to learning about new technologies and applications.</p>",
    "plaintext": "In June, I used my student email address to get a discount on the Network+ certification. I gave myself one month to study and prepare for the exam, but when it came time for my to take it, I was not overly confident. I had read the study guides a few time by this point and set up my own practice network at home.\n\nThe first week of July, I took the Network+ test and passed it. I was so excited that I purchased the Security+ voucher and scheduled the exam for 10 days later. I wanted to build upon my success and was feeling confident. I took and passed the exam. At this point, I knew this was the career path that I wanted to take.\n\nI doubled down on investing in a cyber lab. I bought a used Sophos firewall and three mini pcs off eBay. Two of the mini pcs had ram that was soldered on the board, but the third did not, so I invested in 64 GB or RAM. This is when I discovered Pfsense and Proxmox. Then I deployed several Linux server and an Active Directory domain. I think this was the point in my journey that I became addicted to learning about new technologies and applications.",
    "feature_image": null,
    "excerpt": "In June, I used my student email address to get a discount on the Network+ certification. I gave myself one month to study and prepare for the exam, but when it came time for my to take it, I was not ...",
    "published_at": "2025-10-15T21:51:06.000Z",
    "published_date": "October 15, 2025",
    "tags": [
      "Personal Growth"
    ],
    "featured": false
  },
  {
    "id": "the-plan",
    "title": "The plan",
    "slug": "the-plan",
    "html": "<p>It was now December 2022 and I had a plan coming together.  At the time, I was trying to decide if I would be better off pursuing a bachelor's degree or certifications.  There were several different opinions on this matter and I was not sure.  Finally, I decided that I would do both.  I was determined to commit all of my resources to this pursuit and willing to do anything to ensure I did not fail.</p><p>I had filled out my FASFA and picked out a college.  I was going to enroll in the Cyber Security bachelor's degree program at Champlain College.  I was nervous and excited at the same time.  I had spent so much time over the last few years looking at my past decisions in regret, but never doing anything about it.  I was finally going to take action.</p><p>In April 2023, I enrolled and began classes at Champlain College.  I had been putting off getting my bachelor's degree for a while and I was excited to be moving forward with my life.  I was taking two 7-week classes at once with an average break between classes of one week.  </p><p>I started looking for certifications that were free or affordable.  I came across ISC2's Certified in Cybersecurity and immediately applied for the free voucher.  I began studying that day.  I would read study guides and blog posts.  I also started to listen to every podcast I could find about technology and cyber security.  My favorite podcasts at the time were Cyber Work and the Self-Hosted Show.  </p><p>Later in April, I took the test for the ISC2 CC and passed it.  It wasn't an overly difficult test especially after studying for all the CompTIA certifications, but the I was still proud of the achievement.  It was the snowball that I would build into an avalanche.  I started working through the Cisco Networking Academy and got the badge for Junior Cybersecurity Analyst.</p><p>Then I realized the Microsoft was offering some free fundamentals certifications, but not for very much longer.  I managed to pass the following three Microsoft certifications before July.</p><ol><li>Microsoft Certified: Security, Compliance, and Identity Fundamentals</li><li>Microsoft Certified: Azure AI Fundamentals</li><li>Microsoft Certified: Power Platform Fundamentals</li></ol><p>Once I had these free certifications, I discovered that I got a student discount from CompTIA.  I had a new goal and began pursuing it.</p>",
    "plaintext": "It was now December 2022 and I had a plan coming together. At the time, I was trying to decide if I would be better off pursuing a bachelor's degree or certifications. There were several different opinions on this matter and I was not sure. Finally, I decided that I would do both. I was determined to commit all of my resources to this pursuit and willing to do anything to ensure I did not fail.\n\nI had filled out my FASFA and picked out a college. I was going to enroll in the Cyber Security bachelor's degree program at Champlain College. I was nervous and excited at the same time. I had spent so much time over the last few years looking at my past decisions in regret, but never doing anything about it. I was finally going to take action.\n\nIn April 2023, I enrolled and began classes at Champlain College. I had been putting off getting my bachelor's degree for a while and I was excited to be moving forward with my life. I was taking two 7-week classes at once with an average break between classes of one week.\n\nI started looking for certifications that were free or affordable. I came across ISC2's Certified in Cybersecurity and immediately applied for the free voucher. I began studying that day. I would read study guides and blog posts. I also started to listen to every podcast I could find about technology and cyber security. My favorite podcasts at the time were Cyber Work and the Self-Hosted Show.\n\nLater in April, I took the test for the ISC2 CC and passed it. It wasn't an overly difficult test especially after studying for all the CompTIA certifications, but the I was still proud of the achievement. It was the snowball that I would build into an avalanche. I started working through the Cisco Networking Academy and got the badge for Junior Cybersecurity Analyst.\n\nThen I realized the Microsoft was offering some free fundamentals certifications, but not for very much longer. I managed to pass the following three Microsoft certifications before July.\n\n 1. Microsoft Certified: Security, Compliance, and Identity Fundamentals\n 2. Microsoft Certified: Azure AI Fundamentals\n 3. Microsoft Certified: Power Platform Fundamentals\n\nOnce I had these free certifications, I discovered that I got a student discount from CompTIA. I had a new goal and began pursuing it.",
    "feature_image": null,
    "excerpt": "It was now December 2022 and I had a plan coming together. At the time, I was trying to decide if I would be better off pursuing a bachelor's degree or certifications. There were several different opi...",
    "published_at": "2025-10-15T16:46:31.000Z",
    "published_date": "October 15, 2025",
    "tags": [
      "Personal Growth"
    ],
    "featured": false
  },
  {
    "id": "you-either-evolve-or-die",
    "title": "You either evolve or die.",
    "slug": "you-either-evolve-or-die",
    "html": "<p>It was June 2022, when I realized that I had been living live on auto pilot.  My children were getting older and starting to have their own lives.  It made me wonder what about my life.  I was not where I thought I would be in my life.  I had a lot to be thankful for. I have a family and we are all healthy, but at this point in my life I had given several years to a job that was not going anywhere.  It allowed me to provide for my family, but I felt like I was stuck in an endless loop of mediocrity.  I could go to work and \"excel\", but it required very little cognitively from me.  There were times that I had to solve complex problems, but for the most part every other day was the same.</p><p>I knew at this point that I had to transition into a career that I was passionate about and I knew that was some form of technology.  I had an old computer in the closet, an HP 210 desktop, and I got it out and began experimenting with it.  I wanted to create something that my family would use as well and I ended up turning it into a NAS using TrueNAS Scale.  It took me few weeks deploy it exactly the was I wanted and afterwards I knew that servers and computers were my passion.  There is something about building and troubleshooting a complex system that inspires me.  I think it is the fact that I could spend an incredible amount of time learning about any one system and there would still be more to learn.  It was the exact opposite of my current career.</p><p>When I began looking for a technical career, the one thing that I consistently heard was about a skills gap in cyber security.  I understood basic concepts of cyber security at this time, but did not have a ton of knowledge about it.  I decided that before I was going to invest everything in this path, I wanted to research it some more.  I spent the next six months reading every study guide I could get my hands on.  I didn't realize at this time the vast amount of knowledge that is available online for free.  I read every CompTIA study guide from A+ to CASP+, the CISSP study guide, and the CEH study guide.  I knew then that if I was interested and motivated enough to read all of these study guides, this was a path worth pursuing.</p><p>I began looking at college to attend online so that I could continue to work and support my family.  It was like I was finally alive again professionally.  </p>",
    "plaintext": "It was June 2022, when I realized that I had been living live on auto pilot. My children were getting older and starting to have their own lives. It made me wonder what about my life. I was not where I thought I would be in my life. I had a lot to be thankful for. I have a family and we are all healthy, but at this point in my life I had given several years to a job that was not going anywhere. It allowed me to provide for my family, but I felt like I was stuck in an endless loop of mediocrity. I could go to work and \"excel\", but it required very little cognitively from me. There were times that I had to solve complex problems, but for the most part every other day was the same.\n\nI knew at this point that I had to transition into a career that I was passionate about and I knew that was some form of technology. I had an old computer in the closet, an HP 210 desktop, and I got it out and began experimenting with it. I wanted to create something that my family would use as well and I ended up turning it into a NAS using TrueNAS Scale. It took me few weeks deploy it exactly the was I wanted and afterwards I knew that servers and computers were my passion. There is something about building and troubleshooting a complex system that inspires me. I think it is the fact that I could spend an incredible amount of time learning about any one system and there would still be more to learn. It was the exact opposite of my current career.\n\nWhen I began looking for a technical career, the one thing that I consistently heard was about a skills gap in cyber security. I understood basic concepts of cyber security at this time, but did not have a ton of knowledge about it. I decided that before I was going to invest everything in this path, I wanted to research it some more. I spent the next six months reading every study guide I could get my hands on. I didn't realize at this time the vast amount of knowledge that is available online for free. I read every CompTIA study guide from A+ to CASP+, the CISSP study guide, and the CEH study guide. I knew then that if I was interested and motivated enough to read all of these study guides, this was a path worth pursuing.\n\nI began looking at college to attend online so that I could continue to work and support my family. It was like I was finally alive again professionally.",
    "feature_image": null,
    "excerpt": "It was June 2022, when I realized that I had been living live on auto pilot. My children were getting older and starting to have their own lives. It made me wonder what about my life. I was not where ...",
    "published_at": "2025-10-15T16:35:22.000Z",
    "published_date": "October 15, 2025",
    "tags": [
      "Personal Growth"
    ],
    "featured": false
  },
  {
    "id": "phillips-hue-bridge",
    "title": "Phillips Hue Bridge",
    "slug": "phillips-hue-bridge",
    "html": "<p>I was practicing my Wireshark analysis skills when i came across my Phillips Hue bridge in the capture.  I noticed that it had an http endpoint that I had never seen or used.  The index.html page lists all of the open source software that the hue bridge uses with links to the GitHub pages for each.</p><p>Then looking deeper into the pcap, I noticed the description.xml endpoint.  When I went to this endpoint, I was an information about the device.</p><figure class=\"kg-card kg-image-card\"><img src=\"__GHOST_URL__/content/images/2025/10/hue-description.xml-1.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"715\" height=\"476\" srcset=\"__GHOST_URL__/content/images/size/w600/2025/10/hue-description.xml-1.png 600w, __GHOST_URL__/content/images/2025/10/hue-description.xml-1.png 715w\"></figure><p>  I had the idea then to check for API endpoints.  The /api endpoint did not allow GET requests.  Then i looked to see if i could find any documentation for the REST API.  I found out that you have to push the button on the Hue bridge and then send a request within 20 seconds.  This is what the process for adding the bridge to different applications.  Each application gets a username that it then uses to send requests to the different endpoints that perform different functions, such as turning on and off the lights.  So I prepared the following request and pushed the Hue bridge button before sending it.</p><pre><code class=\"language-bash\"> curl -X POST http://192.168.1.63/api -H \"Content-Type: application/json\" -d '{\"devicetype\":\"testing\"}'</code></pre><p>I received the following response.</p><pre><code class=\"language-bash\">[{\"success\":{\"username\":\"6Z6yeXFzRcZSYwH2g7l2qVe9xRKqR6g4XiBdXRJu\"}}]</code></pre><p>Then I hit the /lights endpoint to get a list of lights using the following command.</p><pre><code>curl http://192.168.1.63/api/6Z6yeXFzRcZSYwH2g7l2qVe9xRKqR6g4XiBdXRJu/lights </code></pre><p>The response on the PowerShell terminal was not very readable so I used the following winget command to download and install jq, a command-line JSON processor.</p><pre><code>PS C:\\Users\\james&gt; winget install jqlang.jq\nFound jq [jqlang.jq] Version 1.8.1\nThis application is licensed to you by its owner.\nMicrosoft is not responsible for, nor does it grant any licenses to, third-party packages.\nDownloading https://github.com/jqlang/jq/releases/download/jq-1.8.1/jq-windows-amd64.exe\n  \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  1002 KB / 1002 KB\nSuccessfully verified installer hash\nStarting package install...\nPath environment variable modified; restart your shell to use the new value.\nCommand line alias added: \"jq\"\nSuccessfully installed</code></pre><p>Then I used piped the lights endpoint request to jq to better format it and received a response that listed all of the Hue lights and information about them.  I have included only one for brevity.</p><pre><code>{\n  \"5\": {\n    \"state\": {\n      \"on\": true,\n      \"bri\": 254,\n      \"hue\": 8417,\n      \"sat\": 140,\n      \"effect\": \"none\",\n      \"xy\": [\n        0.1900,\n        0.0600\n      ],\n      \"ct\": 366,\n      \"alert\": \"select\",\n      \"colormode\": \"ct\",\n      \"mode\": \"homeautomation\",\n      \"reachable\": true\n    },\n    \"swupdate\": {\n      \"state\": \"noupdates\",\n      \"lastinstall\": \"2025-07-21T18:53:24\"\n    },\n    \"type\": \"Extended color light\",\n    \"name\": \"Kids 1\",\n    \"modelid\": \"LCA007\",\n    \"manufacturername\": \"Signify Netherlands B.V.\",\n    \"productname\": \"Hue color lamp\",\n    \"capabilities\": {\n      \"certified\": true,\n      \"control\": {\n        \"mindimlevel\": 200,\n        \"maxlumen\": 1100,\n        \"colorgamuttype\": \"C\",\n        \"colorgamut\": [\n          [\n            0.6915,\n            0.3083\n          ],\n          [\n            0.1700,\n            0.7000\n          ],\n          [\n            0.1532,\n            0.0475\n          ]\n        ],\n        \"ct\": {\n          \"min\": 153,\n          \"max\": 500\n        }\n      },\n      \"streaming\": {\n        \"renderer\": true,\n        \"proxy\": true\n      }\n    },\n    \"config\": {\n      \"archetype\": \"sultanbulb\",\n      \"function\": \"mixed\",\n      \"direction\": \"omnidirectional\",\n      \"startup\": {\n        \"mode\": \"safety\",\n        \"configured\": true\n      }\n    },\n    \"uniqueid\": \"00:17:88:01:0d:b4:76:32-0b\",\n    \"swversion\": \"1.122.8\",\n    \"swconfigid\": \"D11C1AA7\",\n    \"productid\": \"Philips-LCA007-1-A19HECLv1\"\n  },</code></pre><p>Then I went a little bit deeper with the following command to hit the /state endpoint for that light.</p><pre><code>curl http://192.168.1.63/api/6Z6yeXFzRcZSYwH2g7l2qVe9xRKqR6g4XiBdXRJu/lights/5/state | jq</code></pre><p>That gave me the following response and it was then I realized that the state could be retrieved with just the light id endpoint.</p><pre><code>[\n  {\n    \"error\": {\n      \"type\": 3,\n      \"address\": \"/lights/5/state\",\n      \"description\": \"resource, /lights/5/state, not available\"\n    }\n  }\n]</code></pre><p>  The /state endpoint need to receive a PUT request in order to change the state of the light.  I used the following command to test it.</p><pre><code>curl -X PUT http://192.168.1.63/api/6Z6yeXFzRcZSYwH2g7l2qVe9xRKqR6g4XiBdXRJu/lights/5/state -H \"Content-Type\":\"application/json\" -d '{\"on\":\"false\"}'</code></pre><p>After countless tries, I could not get this command to work in Windows.  I tried using Command Prompt instead of PowerShell and I tried calling curl.exe since curl is an alias for Invoke-WebRequest in PowerShell.  Then I converted the command to use the Invoke-WebRequest syntax.  The following command successfully shut the light off.</p><pre><code class=\"language-Powershell\">Invoke-WebRequest -Uri \"http://192.168.1.63/api/6Z6yeXFzRcZSYwH2g7l2qVe9xRKqR6g4XiBdXRJu/lights/5/state\" -Method PUT -Body '{\"on\":false}' -ContentType \"application/json\"</code></pre><p>This would only turn one light on or off at a time though.  I thought about chaining two commands together to turn them both off at once, but after referring to the documentation, realized that I could send commands to groups.  Group 0 refers to all of the Hue lights and using the following command I was able to query the Hue bridge and get a list of my groups.</p><pre><code class=\"language-cmd\">curl http://192.168.1.63/api/6Z6yeXFzRcZSYwH2g7l2qVe9xRKqR6g4XiBdXRJu/groups | jq</code></pre><p>Since jq would not work in PowerShell, I switched back to the Command Prompt and got the following response.</p><pre><code class=\"language-cmd\">  \"82\": {\n    \"name\": \"Kids\",\n    \"lights\": [\n      \"5\",\n      \"6\"\n    ],\n    \"sensors\": [],\n    \"type\": \"Room\",\n    \"state\": {\n      \"all_on\": true,\n      \"any_on\": true\n    },\n    \"recycle\": false,\n    \"class\": \"Living room\",\n    \"action\": {\n      \"on\": true,\n      \"bri\": 254,\n      \"hue\": 8417,\n      \"sat\": 140,\n      \"effect\": \"none\",\n      \"xy\": [\n        0.1900,\n        0.0600\n      ],\n      \"ct\": 366,\n      \"alert\": \"select\",\n      \"colormode\": \"ct\"\n    }</code></pre><p>Then, I had the group name for the kids room and was ready to test it out on them.  I used the following command to turn all of the lights off at once.</p><pre><code class=\"language-PowerShell\">Invoke-WebRequest -Uri \"http://192.168.1.63/api/6Z6yeXFzRcZSYwH2g7l2qVe9xRKqR6g4XiBdXRJu/lights/5/state\" -Method PUT -Body '{\"on\":false}' -ContentType \"application/json\"</code></pre><p>It worked perfectly.  My girls started yelling at me and then I walked them through this entire process.  They weren't really that interested, but I still think it is valuable knowledge for them to have, especially with the prevalence of smart devices.</p><p>I had been thinking about how I could use an NFC tag to turn on and off my desk light.  My plan up until this point was to configure it using Home Assistant.  I might just use these API calls now if I can figure out how to make it toggle instead of calling on or off.</p>",
    "plaintext": "I was practicing my Wireshark analysis skills when i came across my Phillips Hue bridge in the capture. I noticed that it had an http endpoint that I had never seen or used. The index.html page lists all of the open source software that the hue bridge uses with links to the GitHub pages for each.\n\nThen looking deeper into the pcap, I noticed the description.xml endpoint. When I went to this endpoint, I was an information about the device.\n\nI had the idea then to check for API endpoints. The /api endpoint did not allow GET requests. Then i looked to see if i could find any documentation for the REST API. I found out that you have to push the button on the Hue bridge and then send a request within 20 seconds. This is what the process for adding the bridge to different applications. Each application gets a username that it then uses to send requests to the different endpoints that perform different functions, such as turning on and off the lights. So I prepared the following request and pushed the Hue bridge button before sending it.\n\n curl -X POST http://192.168.1.63/api -H \"Content-Type: application/json\" -d '{\"devicetype\":\"testing\"}'\n\nI received the following response.\n\n[{\"success\":{\"username\":\"6Z6yeXFzRcZSYwH2g7l2qVe9xRKqR6g4XiBdXRJu\"}}]\n\nThen I hit the /lights endpoint to get a list of lights using the following command.\n\ncurl http://192.168.1.63/api/6Z6yeXFzRcZSYwH2g7l2qVe9xRKqR6g4XiBdXRJu/lights \n\nThe response on the PowerShell terminal was not very readable so I used the following winget command to download and install jq, a command-line JSON processor.\n\nPS C:\\Users\\james> winget install jqlang.jq\nFound jq [jqlang.jq] Version 1.8.1\nThis application is licensed to you by its owner.\nMicrosoft is not responsible for, nor does it grant any licenses to, third-party packages.\nDownloading https://github.com/jqlang/jq/releases/download/jq-1.8.1/jq-windows-amd64.exe\n  \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  1002 KB / 1002 KB\nSuccessfully verified installer hash\nStarting package install...\nPath environment variable modified; restart your shell to use the new value.\nCommand line alias added: \"jq\"\nSuccessfully installed\n\nThen I used piped the lights endpoint request to jq to better format it and received a response that listed all of the Hue lights and information about them. I have included only one for brevity.\n\n{\n  \"5\": {\n    \"state\": {\n      \"on\": true,\n      \"bri\": 254,\n      \"hue\": 8417,\n      \"sat\": 140,\n      \"effect\": \"none\",\n      \"xy\": [\n        0.1900,\n        0.0600\n      ],\n      \"ct\": 366,\n      \"alert\": \"select\",\n      \"colormode\": \"ct\",\n      \"mode\": \"homeautomation\",\n      \"reachable\": true\n    },\n    \"swupdate\": {\n      \"state\": \"noupdates\",\n      \"lastinstall\": \"2025-07-21T18:53:24\"\n    },\n    \"type\": \"Extended color light\",\n    \"name\": \"Kids 1\",\n    \"modelid\": \"LCA007\",\n    \"manufacturername\": \"Signify Netherlands B.V.\",\n    \"productname\": \"Hue color lamp\",\n    \"capabilities\": {\n      \"certified\": true,\n      \"control\": {\n        \"mindimlevel\": 200,\n        \"maxlumen\": 1100,\n        \"colorgamuttype\": \"C\",\n        \"colorgamut\": [\n          [\n            0.6915,\n            0.3083\n          ],\n          [\n            0.1700,\n            0.7000\n          ],\n          [\n            0.1532,\n            0.0475\n          ]\n        ],\n        \"ct\": {\n          \"min\": 153,\n          \"max\": 500\n        }\n      },\n      \"streaming\": {\n        \"renderer\": true,\n        \"proxy\": true\n      }\n    },\n    \"config\": {\n      \"archetype\": \"sultanbulb\",\n      \"function\": \"mixed\",\n      \"direction\": \"omnidirectional\",\n      \"startup\": {\n        \"mode\": \"safety\",\n        \"configured\": true\n      }\n    },\n    \"uniqueid\": \"00:17:88:01:0d:b4:76:32-0b\",\n    \"swversion\": \"1.122.8\",\n    \"swconfigid\": \"D11C1AA7\",\n    \"productid\": \"Philips-LCA007-1-A19HECLv1\"\n  },\n\nThen I went a little bit deeper with the following command to hit the /state endpoint for that light.\n\ncurl http://192.168.1.63/api/6Z6yeXFzRcZSYwH2g7l2qVe9xRKqR6g4XiBdXRJu/lights/5/state | jq\n\nThat gave me the following response and it was then I realized that the state could be retrieved with just the light id endpoint.\n\n[\n  {\n    \"error\": {\n      \"type\": 3,\n      \"address\": \"/lights/5/state\",\n      \"description\": \"resource, /lights/5/state, not available\"\n    }\n  }\n]\n\nThe /state endpoint need to receive a PUT request in order to change the state of the light. I used the following command to test it.\n\ncurl -X PUT http://192.168.1.63/api/6Z6yeXFzRcZSYwH2g7l2qVe9xRKqR6g4XiBdXRJu/lights/5/state -H \"Content-Type\":\"application/json\" -d '{\"on\":\"false\"}'\n\nAfter countless tries, I could not get this command to work in Windows. I tried using Command Prompt instead of PowerShell and I tried calling curl.exe since curl is an alias for Invoke-WebRequest in PowerShell. Then I converted the command to use the Invoke-WebRequest syntax. The following command successfully shut the light off.\n\nInvoke-WebRequest -Uri \"http://192.168.1.63/api/6Z6yeXFzRcZSYwH2g7l2qVe9xRKqR6g4XiBdXRJu/lights/5/state\" -Method PUT -Body '{\"on\":false}' -ContentType \"application/json\"\n\nThis would only turn one light on or off at a time though. I thought about chaining two commands together to turn them both off at once, but after referring to the documentation, realized that I could send commands to groups. Group 0 refers to all of the Hue lights and using the following command I was able to query the Hue bridge and get a list of my groups.\n\ncurl http://192.168.1.63/api/6Z6yeXFzRcZSYwH2g7l2qVe9xRKqR6g4XiBdXRJu/groups | jq\n\nSince jq would not work in PowerShell, I switched back to the Command Prompt and got the following response.\n\n  \"82\": {\n    \"name\": \"Kids\",\n    \"lights\": [\n      \"5\",\n      \"6\"\n    ],\n    \"sensors\": [],\n    \"type\": \"Room\",\n    \"state\": {\n      \"all_on\": true,\n      \"any_on\": true\n    },\n    \"recycle\": false,\n    \"class\": \"Living room\",\n    \"action\": {\n      \"on\": true,\n      \"bri\": 254,\n      \"hue\": 8417,\n      \"sat\": 140,\n      \"effect\": \"none\",\n      \"xy\": [\n        0.1900,\n        0.0600\n      ],\n      \"ct\": 366,\n      \"alert\": \"select\",\n      \"colormode\": \"ct\"\n    }\n\nThen, I had the group name for the kids room and was ready to test it out on them. I used the following command to turn all of the lights off at once.\n\nInvoke-WebRequest -Uri \"http://192.168.1.63/api/6Z6yeXFzRcZSYwH2g7l2qVe9xRKqR6g4XiBdXRJu/lights/5/state\" -Method PUT -Body '{\"on\":false}' -ContentType \"application/json\"\n\nIt worked perfectly. My girls started yelling at me and then I walked them through this entire process. They weren't really that interested, but I still think it is valuable knowledge for them to have, especially with the prevalence of smart devices.\n\nI had been thinking about how I could use an NFC tag to turn on and off my desk light. My plan up until this point was to configure it using Home Assistant. I might just use these API calls now if I can figure out how to make it toggle instead of calling on or off.",
    "feature_image": null,
    "excerpt": "I was practicing my Wireshark analysis skills when i came across my Phillips Hue bridge in the capture. I noticed that it had an http endpoint that I had never seen or used. The index.html page lists ...",
    "published_at": "2025-10-12T03:21:44.000Z",
    "published_date": "October 12, 2025",
    "tags": [
      "Home-Lab"
    ],
    "featured": false
  },
  {
    "id": "my-home-lab-addiction",
    "title": "My Home Lab Addiction",
    "slug": "my-home-lab-addiction",
    "html": "<p>I started building my home lab four years ago with an HP 210 Desktop PC.  It was over 5 years old at the time and was barely running Windows anymore, so I converted it to TrueNAS Scale and began the journey down the rabbit hole of home labbing.</p><p>Since then I have spent more money than I would like to admit and definitely enough that some people would look at me like I am crazy.  I don't have a world class home lab though.  About have of my lab was purchased used and the other half I made payment in order to afford it.  There were some pieces that I made an impulsive decision to buy which is why I have begun to refer to my home lab as an addiction.</p><p>The money doesn't really compare to the amount of time that I have invested in it though.  I have spent countless days building, troubleshooting, and breaking so many different platforms.  There are so many applications that seemed easy to deploy when reading the documentation, but then turned out to be anything but easy.  In the first year, I took down my network so many times that my kids still blame me during an outage.</p><p>I would not trade it for anything though.  Even if I never get my dream job, it will have been worth every frustration and penny.  I had kids at a young age, 19, and for so long I was only focused on them.  I got a job that paid the bills, but it was not a career and I was never truly happy there.  There was only so much that I could learn and then it was just the same thing day after day.  That is how I know that information technology and cyber security is where I belong.  The amount of knowledge there is to gain is a bottom-less pit.  I could keep learning for several lifetimes and it would never get stale.</p><p>I have been trying to get a job in cyber security for a few years now and some days I wonder if I ever will.  That was my only goal at first, but as I invest more time and money into it, I realized that doesn't matter as much as the knowledge I gain.  I don't need a hiring manager to validate my knowledge.  I just need to continue to show up day after day to learn and grow.</p><p>I haven't posted any pictures of my lab before because it is not perfect yet.  I don't have a fancy server rack or even a server rack.  My cable management looks like I just cram cables everywhere, but that doesn't really matter.  What matters is that I build, learn, teardown, and start the process over again.</p><figure class=\"kg-card kg-image-card\"><img src=\"__GHOST_URL__/content/images/2025/10/fix-the-lines-on-the.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1536\" height=\"1024\" srcset=\"__GHOST_URL__/content/images/size/w600/2025/10/fix-the-lines-on-the.png 600w, __GHOST_URL__/content/images/size/w1000/2025/10/fix-the-lines-on-the.png 1000w, __GHOST_URL__/content/images/2025/10/fix-the-lines-on-the.png 1536w\" sizes=\"(min-width: 720px) 720px\"></figure><figure class=\"kg-card kg-image-card\"><img src=\"__GHOST_URL__/content/images/2025/10/IMG_20251011_163512306_HDR.jpg\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"1500\" srcset=\"__GHOST_URL__/content/images/size/w600/2025/10/IMG_20251011_163512306_HDR.jpg 600w, __GHOST_URL__/content/images/size/w1000/2025/10/IMG_20251011_163512306_HDR.jpg 1000w, __GHOST_URL__/content/images/size/w1600/2025/10/IMG_20251011_163512306_HDR.jpg 1600w, __GHOST_URL__/content/images/size/w2400/2025/10/IMG_20251011_163512306_HDR.jpg 2400w\" sizes=\"(min-width: 720px) 720px\"></figure>",
    "plaintext": "I started building my home lab four years ago with an HP 210 Desktop PC. It was over 5 years old at the time and was barely running Windows anymore, so I converted it to TrueNAS Scale and began the journey down the rabbit hole of home labbing.\n\nSince then I have spent more money than I would like to admit and definitely enough that some people would look at me like I am crazy. I don't have a world class home lab though. About have of my lab was purchased used and the other half I made payment in order to afford it. There were some pieces that I made an impulsive decision to buy which is why I have begun to refer to my home lab as an addiction.\n\nThe money doesn't really compare to the amount of time that I have invested in it though. I have spent countless days building, troubleshooting, and breaking so many different platforms. There are so many applications that seemed easy to deploy when reading the documentation, but then turned out to be anything but easy. In the first year, I took down my network so many times that my kids still blame me during an outage.\n\nI would not trade it for anything though. Even if I never get my dream job, it will have been worth every frustration and penny. I had kids at a young age, 19, and for so long I was only focused on them. I got a job that paid the bills, but it was not a career and I was never truly happy there. There was only so much that I could learn and then it was just the same thing day after day. That is how I know that information technology and cyber security is where I belong. The amount of knowledge there is to gain is a bottom-less pit. I could keep learning for several lifetimes and it would never get stale.\n\nI have been trying to get a job in cyber security for a few years now and some days I wonder if I ever will. That was my only goal at first, but as I invest more time and money into it, I realized that doesn't matter as much as the knowledge I gain. I don't need a hiring manager to validate my knowledge. I just need to continue to show up day after day to learn and grow.\n\nI haven't posted any pictures of my lab before because it is not perfect yet. I don't have a fancy server rack or even a server rack. My cable management looks like I just cram cables everywhere, but that doesn't really matter. What matters is that I build, learn, teardown, and start the process over again.",
    "feature_image": null,
    "excerpt": "I started building my home lab four years ago with an HP 210 Desktop PC. It was over 5 years old at the time and was barely running Windows anymore, so I converted it to TrueNAS Scale and began the jo...",
    "published_at": "2025-10-11T21:33:11.000Z",
    "published_date": "October 11, 2025",
    "tags": [
      "Home-Lab"
    ],
    "featured": false
  },
  {
    "id": "proxmox-cyber-lab",
    "title": "Proxmox Cyber Lab",
    "slug": "proxmox-cyber-lab",
    "html": "<p>I have been working on recreating my cyber lab and I found a solution that I believe I will stick with.  I have several VMs on Proxmox that are all connected to an internal network.  I was using a Kali VM on Proxmox for all of my testing, but I wanted to be able to reach the cyber lab from different machines.  I had been using Tailscale for my VPN for a while, but had never thought of using it for my cyber lab until today.</p><p>The first step was to create a Windows 2019 Server with a desktop and then promote it to a domain controller.  I took a backup snapshot at this point, so I could always go back to a known good configuration.  Then I created a Windows 2022 Server without a desktop and promoted it to a domain controller for the same domain.  I had never performed this without a GUI before so it was a learning experience for me.  Using the console, it was trivial to change the computer name, setup the network, and add it to the domain.  Afterwards, I used PowerShell to promote it to a domain controller using the following commands:</p><pre><code class=\"language-Powershell\">$credential = Get-Credential\n$safeModePassword = Get-Credential\nInstall-ADDSDomainController -DomainName \"corp.lan\" -Credential $credential -SafeModeAdministratorPassword $safeModePassword</code></pre><p>I wanted to create a CLI domain controller to help me learn Active Directory Powershell commands.  I used to think it was easier to manage a Domain Controller with the GUI, but using PowerShell-Remoting or Invoke-Command makes it faster and easier.</p><p>Then, I created a Windows 10 VM and added it to the domain.  Afterwards, I created a Linux VM and installed Tailscale on it.  I setup Tailscale to route traffic to the internal subnet.  I went into the Tailscale console and configured the DNS server for the corp.lan domain to be the first domain controller.  This allows me to use DNS names instead of IP address.</p><p>I  wanted to add some services that I knew were vulnerable as well so, I downloaded the Metasploitable 2 VM.  I then uploaded it to Proxmox, uncompressed the disk, and converted it to qemu.  I wanted Metasploitable 3 as well, but it took a little more effort.  I went through the steps of using Vagrant to create the VM and then uploaded it the same way.</p><p>Now I have a basic cyber lab that I can use for testing.  Installing Tailscale not only allows me to test with other machines, but as to access my lab from anywhere.  The next project for my lab is to create another domain in the Active Directory forest and build a trust relationship between them.</p>",
    "plaintext": "I have been working on recreating my cyber lab and I found a solution that I believe I will stick with. I have several VMs on Proxmox that are all connected to an internal network. I was using a Kali VM on Proxmox for all of my testing, but I wanted to be able to reach the cyber lab from different machines. I had been using Tailscale for my VPN for a while, but had never thought of using it for my cyber lab until today.\n\nThe first step was to create a Windows 2019 Server with a desktop and then promote it to a domain controller. I took a backup snapshot at this point, so I could always go back to a known good configuration. Then I created a Windows 2022 Server without a desktop and promoted it to a domain controller for the same domain. I had never performed this without a GUI before so it was a learning experience for me. Using the console, it was trivial to change the computer name, setup the network, and add it to the domain. Afterwards, I used PowerShell to promote it to a domain controller using the following commands:\n\n$credential = Get-Credential\n$safeModePassword = Get-Credential\nInstall-ADDSDomainController -DomainName \"corp.lan\" -Credential $credential -SafeModeAdministratorPassword $safeModePassword\n\nI wanted to create a CLI domain controller to help me learn Active Directory Powershell commands. I used to think it was easier to manage a Domain Controller with the GUI, but using PowerShell-Remoting or Invoke-Command makes it faster and easier.\n\nThen, I created a Windows 10 VM and added it to the domain. Afterwards, I created a Linux VM and installed Tailscale on it. I setup Tailscale to route traffic to the internal subnet. I went into the Tailscale console and configured the DNS server for the corp.lan domain to be the first domain controller. This allows me to use DNS names instead of IP address.\n\nI wanted to add some services that I knew were vulnerable as well so, I downloaded the Metasploitable 2 VM. I then uploaded it to Proxmox, uncompressed the disk, and converted it to qemu. I wanted Metasploitable 3 as well, but it took a little more effort. I went through the steps of using Vagrant to create the VM and then uploaded it the same way.\n\nNow I have a basic cyber lab that I can use for testing. Installing Tailscale not only allows me to test with other machines, but as to access my lab from anywhere. The next project for my lab is to create another domain in the Active Directory forest and build a trust relationship between them.",
    "feature_image": null,
    "excerpt": "I have been working on recreating my cyber lab and I found a solution that I believe I will stick with. I have several VMs on Proxmox that are all connected to an internal network. I was using a Kali ...",
    "published_at": "2025-10-02T00:21:01.000Z",
    "published_date": "October 02, 2025",
    "tags": [
      "Cyber Security"
    ],
    "featured": false
  },
  {
    "id": "tcm-security-pwpa",
    "title": "TCM Security PWPA",
    "slug": "tcm-security-pwpa",
    "html": "<p>I bought a voucher for the PWPA a couple of months ago and have been practicing on their docker lab for a while now.  Last weekend, I decided that I would go ahead and give the test a try.  I had a retest so failing the first attempt would not be the worst thing.  It would at least give me some practice and insight.  This is the same approach that I took with the CompTIA exams and it worked for me then.</p><p>Before I started the test, I was nervous.  Last December, I took the Hack the Box CBBH and completely bombed it.  I got three out of six goals completed and was taking shots in the dark for most of the time.  I even took off work to give myself more time with the exam.  I spent over 10 hours a day for seven days, just throwing anything I could think of at it.  I didn't even attempt to write the report.  I hindsight, I should have so that I could get some feedback.  I took the test a two weeks before Christmas and some how when the email saying that I failed came in I missed it.  With the Hack the Box exam, you only have two weeks after the failure notification to start the exam again.  I feel like I checked my email ten times a day until after the new year and still did not see the notification.  So going into the PWPA, I was not overly confident.</p><p>I am still waiting to get the test results back, but I feel like I did great on it.  I started of flailing around like I did on the CBBH, but then I took a step back and began just using the app instead of trying to break it.  I went through all of the functionality and tried to hit every endpoint.  Afterwards, I looked back at the HTTP history in Burp Suite and began to pick out requests that looked promising.  I sent all of them to the Organizer tab to save for later and then went through them one by one sending them to Repeater.  If I found something interesting in Repeater but couldn't exploit it, I would send it to Intruder.</p><p>This worked fantastic for me.  I had a focused and organized work flow that allowed me to group requests together and search for specific types of vulnerabilities on each of them.  I doubt that I found all of the exploits, but I found enough to pass the exam.  I quickly went to work on the report and had what I felt like was a good report ready before the end of the second day.   The exam gives you four days, two for testing and two for the report, but I finished it in two days, one for testing and one for reporting.  I am currently waiting to get my results back, but will post an update to this page when I do.  The waiting to see if I passed or failed is one of the hardest parts for me.  I know it might take a couple of days, but I can't help from checking my email over and over again, even though I have notifications turned on.</p><p>Hopefully, someone reads this and doesn't feel so lost trying to break into cyber security.  It is a big ocean and it is easy to feel like you are lost at sea, just drifting around.  I have not achieved my goals yet, but I am dedicated and unwilling to accept anything other than success, no matter how many failures it is built upon.</p><p>It has taken me long than I hoped to finish this post, but I aced (I think) the PWPA exam. I received the email congratulating me for passing just two weekdays after taking the exam.  If anyone else is think of buying this exam, I would recommend it.  It was not an overly difficult exam, but it helps to solidify your understanding of basic web vulnerabilities.</p>",
    "plaintext": "I bought a voucher for the PWPA a couple of months ago and have been practicing on their docker lab for a while now. Last weekend, I decided that I would go ahead and give the test a try. I had a retest so failing the first attempt would not be the worst thing. It would at least give me some practice and insight. This is the same approach that I took with the CompTIA exams and it worked for me then.\n\nBefore I started the test, I was nervous. Last December, I took the Hack the Box CBBH and completely bombed it. I got three out of six goals completed and was taking shots in the dark for most of the time. I even took off work to give myself more time with the exam. I spent over 10 hours a day for seven days, just throwing anything I could think of at it. I didn't even attempt to write the report. I hindsight, I should have so that I could get some feedback. I took the test a two weeks before Christmas and some how when the email saying that I failed came in I missed it. With the Hack the Box exam, you only have two weeks after the failure notification to start the exam again. I feel like I checked my email ten times a day until after the new year and still did not see the notification. So going into the PWPA, I was not overly confident.\n\nI am still waiting to get the test results back, but I feel like I did great on it. I started of flailing around like I did on the CBBH, but then I took a step back and began just using the app instead of trying to break it. I went through all of the functionality and tried to hit every endpoint. Afterwards, I looked back at the HTTP history in Burp Suite and began to pick out requests that looked promising. I sent all of them to the Organizer tab to save for later and then went through them one by one sending them to Repeater. If I found something interesting in Repeater but couldn't exploit it, I would send it to Intruder.\n\nThis worked fantastic for me. I had a focused and organized work flow that allowed me to group requests together and search for specific types of vulnerabilities on each of them. I doubt that I found all of the exploits, but I found enough to pass the exam. I quickly went to work on the report and had what I felt like was a good report ready before the end of the second day. The exam gives you four days, two for testing and two for the report, but I finished it in two days, one for testing and one for reporting. I am currently waiting to get my results back, but will post an update to this page when I do. The waiting to see if I passed or failed is one of the hardest parts for me. I know it might take a couple of days, but I can't help from checking my email over and over again, even though I have notifications turned on.\n\nHopefully, someone reads this and doesn't feel so lost trying to break into cyber security. It is a big ocean and it is easy to feel like you are lost at sea, just drifting around. I have not achieved my goals yet, but I am dedicated and unwilling to accept anything other than success, no matter how many failures it is built upon.\n\nIt has taken me long than I hoped to finish this post, but I aced (I think) the PWPA exam. I received the email congratulating me for passing just two weekdays after taking the exam. If anyone else is think of buying this exam, I would recommend it. It was not an overly difficult exam, but it helps to solidify your understanding of basic web vulnerabilities.",
    "feature_image": null,
    "excerpt": "I bought a voucher for the PWPA a couple of months ago and have been practicing on their docker lab for a while now. Last weekend, I decided that I would go ahead and give the test a try. I had a rete...",
    "published_at": "2025-08-25T15:41:09.000Z",
    "published_date": "August 25, 2025",
    "tags": [
      "Personal Growth"
    ],
    "featured": false
  },
  {
    "id": "home-lab-redesign-pt-2",
    "title": "Home Lab Redesign pt. 2",
    "slug": "home-lab-redesign-pt-2",
    "html": "<p>In the last post, I discussed the hardware that I am using. In this post, I will dive into the network layout, but first I want to discuss the various ways to keep track of network information.  Once you get your network setup, it will be hard to keep track of what service is at which IP address.  A reverse proxy helps to solve some of these issues, but then if the proxy goes down you are stuck trying to find the IP address again.</p><p>The first option that I used was just a pen and a piece of paper, it was actually a sticky note.  I began to write the network address on the top and then the host addresses by the service name.  This worked okay at first, but if I had any changes my sticky note would be covered in scribbles or I would have to start a new note and copy everything over to it.  It didn't take me long to realize that this was not a good method.  I then started a note in Trillium Notes that allowed me to make modifications without starting over.</p><figure class=\"kg-card kg-image-card\"><img src=\"__GHOST_URL__/content/images/2025/08/image-1.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"240\" height=\"286\"></figure><p>I wanted to familiarize myself with more tools so, I started using phpIPAM.  It took me a few tries to get it setup at first, but once I got it I began to prefer it over my note.  I can dynamically add IP addresses and monitors if they are still up.  It can also query DNS servers to get the FQDN (Fully Qualified Domain Name) of services.  It had a lot of things that I did not use such as different customers and locations, but it had a nice layout and made it easy to find the IP address of services.  It was a large improvement over the sticky and digital notes.</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"__GHOST_URL__/content/images/2025/08/image.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1066\" height=\"1150\" srcset=\"__GHOST_URL__/content/images/size/w600/2025/08/image.png 600w, __GHOST_URL__/content/images/size/w1000/2025/08/image.png 1000w, __GHOST_URL__/content/images/2025/08/image.png 1066w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">phpIPAM</span></figcaption></figure><p>The next option that I tried was Netbox.  I thought that phpIPAM had a lot of features that I was not using, but Netbox has even more.  It allows you to thoroughly document your network and networked devices.  It has several different sections that breakdown into smaller sections.  In the device section, it allows you to define manufacturers, device roles, type of device, and even the direction of airflow.</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"__GHOST_URL__/content/images/2025/08/image-2.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1058\" height=\"1618\" srcset=\"__GHOST_URL__/content/images/size/w600/2025/08/image-2.png 600w, __GHOST_URL__/content/images/size/w1000/2025/08/image-2.png 1000w, __GHOST_URL__/content/images/2025/08/image-2.png 1058w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Netbox</span></figcaption></figure><p>I have not thoroughly document my devices in Netbox yet, but am in the process of building out my documentation.  I think that Netbox will not only help me to keep track of IP address, but also keep track of hardware and configurations.  It allows me to document what port on my switches maps to which device and much more.  I plan on using the inventory that I build out in Netbox to help me in the redesign process.  I will admit that I have not used Netbox as much as phpIPAM, so I am not an expert.  I do think that it does have potential though.  I will probably make a different blog post to document Netbox once I figure out all the different features.</p><p>I am using Proxmox's IPAM for virtual machines currently running on Proxmox.  At first, I liked this setup because all of the information was in Proxmox's Web UI, but I am going to migrate everything to the same place.  There is a way to use Netbox as Proxmox's IPAM, but I have not gotten it to work yet.  I have used phpIPAM as Proxmox's IPAM.</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"__GHOST_URL__/content/images/2025/08/image-3.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"747\" height=\"1009\" srcset=\"__GHOST_URL__/content/images/size/w600/2025/08/image-3.png 600w, __GHOST_URL__/content/images/2025/08/image-3.png 747w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Proxmox</span></figcaption></figure><p>I am going to use Netbox to design my network.  I am going to start at a high level.  I only have one internet connection that connects to Opnsense.  From Opnsense, I am going to split into two switches.  The PoE+ switch will be used for my home network and the Cisco switch will be used for my home lab.  The home network is going to be on the 192.168.0.0/24 and my home lab network will be 10.0.0.0/24.  I am also going to add a virtual network to Proxmox that will be used to connect the vulnerable services like Metasploitable3 and Kali Linux.  I was going to cut down to just one switch since I am currently only using 22 ports, but I decided to keep the switches and then I can create a SPAN port on the Cisco switch to capture traffic.  This will allow me to ensure that my children aren't complaining because the IPS/IDS is slowing down the network and I won't capture a bunch of traffic from You tube or from the PlayStation.</p><p>I will use link aggregation and LACP to use two ports connecting the switches and Opnsense.  I will also use LACP between the Cisco switch and Proxmox host.  I have been using LACP to connect my Synology and Active Directory to my home lab network.  If I still use this setup, I will need to allow some traffic to traverse the subnets.  I want to have Active Directory and Synology access on both networks.  I have been forwarding all traffic between the two subnets, so I will need to make a list of ports that are allowed to traverse the subnets.  Instead of using bonded ports, I could split the connections up between the subnets.  Then I would not need to forward traffic for these services.  They would just use the proper interface to connect to either network.  I think that with the level of traffic in my network LACP isn't really necessary.</p><p>In the next post, I will discuss installing and configuring Proxmox and begin detailing the different services that I am using.  I am going to try to make a blog post for each service that I use.  I will document the configurations and try to make it as repeatable of a process as I can.  I have found Docker containers for most of the services and use Docker Compose to deploy them, so it should be a fairly easy copy, paste, compose up.</p>",
    "plaintext": "In the last post, I discussed the hardware that I am using. In this post, I will dive into the network layout, but first I want to discuss the various ways to keep track of network information. Once you get your network setup, it will be hard to keep track of what service is at which IP address. A reverse proxy helps to solve some of these issues, but then if the proxy goes down you are stuck trying to find the IP address again.\n\nThe first option that I used was just a pen and a piece of paper, it was actually a sticky note. I began to write the network address on the top and then the host addresses by the service name. This worked okay at first, but if I had any changes my sticky note would be covered in scribbles or I would have to start a new note and copy everything over to it. It didn't take me long to realize that this was not a good method. I then started a note in Trillium Notes that allowed me to make modifications without starting over.\n\nI wanted to familiarize myself with more tools so, I started using phpIPAM. It took me a few tries to get it setup at first, but once I got it I began to prefer it over my note. I can dynamically add IP addresses and monitors if they are still up. It can also query DNS servers to get the FQDN (Fully Qualified Domain Name) of services. It had a lot of things that I did not use such as different customers and locations, but it had a nice layout and made it easy to find the IP address of services. It was a large improvement over the sticky and digital notes.\n\nThe next option that I tried was Netbox. I thought that phpIPAM had a lot of features that I was not using, but Netbox has even more. It allows you to thoroughly document your network and networked devices. It has several different sections that breakdown into smaller sections. In the device section, it allows you to define manufacturers, device roles, type of device, and even the direction of airflow.\n\nI have not thoroughly document my devices in Netbox yet, but am in the process of building out my documentation. I think that Netbox will not only help me to keep track of IP address, but also keep track of hardware and configurations. It allows me to document what port on my switches maps to which device and much more. I plan on using the inventory that I build out in Netbox to help me in the redesign process. I will admit that I have not used Netbox as much as phpIPAM, so I am not an expert. I do think that it does have potential though. I will probably make a different blog post to document Netbox once I figure out all the different features.\n\nI am using Proxmox's IPAM for virtual machines currently running on Proxmox. At first, I liked this setup because all of the information was in Proxmox's Web UI, but I am going to migrate everything to the same place. There is a way to use Netbox as Proxmox's IPAM, but I have not gotten it to work yet. I have used phpIPAM as Proxmox's IPAM.\n\nI am going to use Netbox to design my network. I am going to start at a high level. I only have one internet connection that connects to Opnsense. From Opnsense, I am going to split into two switches. The PoE+ switch will be used for my home network and the Cisco switch will be used for my home lab. The home network is going to be on the 192.168.0.0/24 and my home lab network will be 10.0.0.0/24. I am also going to add a virtual network to Proxmox that will be used to connect the vulnerable services like Metasploitable3 and Kali Linux. I was going to cut down to just one switch since I am currently only using 22 ports, but I decided to keep the switches and then I can create a SPAN port on the Cisco switch to capture traffic. This will allow me to ensure that my children aren't complaining because the IPS/IDS is slowing down the network and I won't capture a bunch of traffic from You tube or from the PlayStation.\n\nI will use link aggregation and LACP to use two ports connecting the switches and Opnsense. I will also use LACP between the Cisco switch and Proxmox host. I have been using LACP to connect my Synology and Active Directory to my home lab network. If I still use this setup, I will need to allow some traffic to traverse the subnets. I want to have Active Directory and Synology access on both networks. I have been forwarding all traffic between the two subnets, so I will need to make a list of ports that are allowed to traverse the subnets. Instead of using bonded ports, I could split the connections up between the subnets. Then I would not need to forward traffic for these services. They would just use the proper interface to connect to either network. I think that with the level of traffic in my network LACP isn't really necessary.\n\nIn the next post, I will discuss installing and configuring Proxmox and begin detailing the different services that I am using. I am going to try to make a blog post for each service that I use. I will document the configurations and try to make it as repeatable of a process as I can. I have found Docker containers for most of the services and use Docker Compose to deploy them, so it should be a fairly easy copy, paste, compose up.",
    "feature_image": null,
    "excerpt": "In the last post, I discussed the hardware that I am using. In this post, I will dive into the network layout, but first I want to discuss the various ways to keep track of network information. Once y...",
    "published_at": "2025-08-16T23:16:21.000Z",
    "published_date": "August 16, 2025",
    "tags": [
      "Home-Lab"
    ],
    "featured": false
  },
  {
    "id": "home-lab-redesign-pt-1-hardware",
    "title": "Home Lab Redesign pt. 1 Hardware",
    "slug": "home-lab-redesign-pt-1-hardware",
    "html": "<p>The first part of my home lab that I am going to map out is the hardware.  I think that having a pre-defined plan for the hardware will help prevent me from sprawling apps across various machines.  Most of my hardware was bought second hand off of eBay and most of it is consumer equipment.  My home lab will feature the following hardware: </p><ul><li>APC UPS</li><li>Sophos firewall (running Opnsense)</li><li>HP Aruba PoE+ managed switch</li><li>Cisco Business managed switch</li><li>Unifi Ubiquiti Access Point</li><li>Synology NAS</li><li>Minisforum MS-A1</li><li>Minisforum TH50</li><li>Beelink SER5 Max</li><li>Nvidia Jetson Orin Nano</li><li>Raspberry Pi 5</li><li>Hue Bridge</li><li>2 Reolink cameras</li></ul><p>I have a few Intel NUCs and a couple of Raspberry Pi Zero2Ws, but I am not sure that I am going to be using them.  I bought them before most of the other hardware while I was on a hardware buying spree.  At first, I was only going to buy cheap hardware and use it for a cyber lab, but once I realized the potential I bought better hardware.  Now I have a couple of shelves full of hardware that I might never use, but don't want to get rid of just in case I do find a use case for them.  I also have an assortment of monitors, keyboards, mice, and more ethernet cable than I will probably ever use.</p><p>All of my network equipment is 1 Gbps, so what I have been doing for the core devices is using link aggregation and LACP.  This allows me to give two ports the same IP address and lets the systems alternate between the two depending on the available bandwidth.  It makes my network 2 Gbps between these devices, although it does not give a single data stream more bandwidth.  Instead it distributes multiple data streams across the 2 Gbps connection.   I have two links from my firewall to both switches and from the Cisco Switch to both Minisforum pcs and the Synology NAS.</p><p>In the past, I have separated my home internet from my home lab with two different switches.  I experimented with VLANs at first, but switched to separating the networks on different hardware because it turns out my children don't like it when I bring the internet down.  They have taken to calling me a tinkerer because of the number of times that I have made the internet unusable.  I wasn't aware at first of all the ways that I could cripple our internet connection.</p><p>In the next post, I will document the Network Layout including ports and IP addresses.  I will also discuss the various methods that I have used to keep track of this information.</p><h4 id=\"\"></h4><p></p>",
    "plaintext": "The first part of my home lab that I am going to map out is the hardware. I think that having a pre-defined plan for the hardware will help prevent me from sprawling apps across various machines. Most of my hardware was bought second hand off of eBay and most of it is consumer equipment. My home lab will feature the following hardware:\n\n * APC UPS\n * Sophos firewall (running Opnsense)\n * HP Aruba PoE+ managed switch\n * Cisco Business managed switch\n * Unifi Ubiquiti Access Point\n * Synology NAS\n * Minisforum MS-A1\n * Minisforum TH50\n * Beelink SER5 Max\n * Nvidia Jetson Orin Nano\n * Raspberry Pi 5\n * Hue Bridge\n * 2 Reolink cameras\n\nI have a few Intel NUCs and a couple of Raspberry Pi Zero2Ws, but I am not sure that I am going to be using them. I bought them before most of the other hardware while I was on a hardware buying spree. At first, I was only going to buy cheap hardware and use it for a cyber lab, but once I realized the potential I bought better hardware. Now I have a couple of shelves full of hardware that I might never use, but don't want to get rid of just in case I do find a use case for them. I also have an assortment of monitors, keyboards, mice, and more ethernet cable than I will probably ever use.\n\nAll of my network equipment is 1 Gbps, so what I have been doing for the core devices is using link aggregation and LACP. This allows me to give two ports the same IP address and lets the systems alternate between the two depending on the available bandwidth. It makes my network 2 Gbps between these devices, although it does not give a single data stream more bandwidth. Instead it distributes multiple data streams across the 2 Gbps connection. I have two links from my firewall to both switches and from the Cisco Switch to both Minisforum pcs and the Synology NAS.\n\nIn the past, I have separated my home internet from my home lab with two different switches. I experimented with VLANs at first, but switched to separating the networks on different hardware because it turns out my children don't like it when I bring the internet down. They have taken to calling me a tinkerer because of the number of times that I have made the internet unusable. I wasn't aware at first of all the ways that I could cripple our internet connection.\n\nIn the next post, I will document the Network Layout including ports and IP addresses. I will also discuss the various methods that I have used to keep track of this information.\n\n\n\n",
    "feature_image": null,
    "excerpt": "The first part of my home lab that I am going to map out is the hardware. I think that having a pre-defined plan for the hardware will help prevent me from sprawling apps across various machines. Most...",
    "published_at": "2025-08-14T15:24:39.000Z",
    "published_date": "August 14, 2025",
    "tags": [
      "Home-Lab"
    ],
    "featured": false
  },
  {
    "id": "home-lab-pt-1",
    "title": "Home Lab",
    "slug": "home-lab-pt-1",
    "html": "<p>When I began building my home lab, I had no idea how much it would help me out in certifications and at work.  I have been able to use my home lab as a testing server to learn how to setup and configure different apps and services.  In this post and subsequent posts I will detail how I ended up becoming an avid homelabber and any advice that I have for others thinking about pursuing a home lab.</p><p>It all started two years ago with an install of Truenas Scale.  The ability to run containers on my desktop computer that I converted into a NAS was what inspired me to go down the self-hosted rabbit hole.  With these containers, I was introduced to Plex and Tailscale.  The introduction to Plex inspired me to utilize other containers, but Tailscale was the real service that inspired my home lab.  I discovered how easy it was to utilize and build Wireguard tunnels between my devices.  It was this mobility that inspired me to try other services.  If I had to be physically at home to use my home lab, I probably would not have made it this far.</p><p>The first hardware that I purchased for my home lab was an Intel NUC I bought for $50 on Ebay.  Then, I began experimenting with different type-1(bare-metal) hypervisors.  I started with Truenas Scale because it was what I was familiar with, but I tried several different hypervisors.  In the end, I stuck with Proxmox because the GUI was organized in a way that made it easy to manage virtual machines and containers.  </p><p>Once I discovered the ability to cluster Proxmox hosts, I bought another Intel NUC and a Minisforum TH50.  I used the NUCs to create a Proxmox cluster.  At first I had various services on each host, but then I began to experiment with CEPH storage and high-availability.  It gave me some appreciation to the complexities involved with designing and implementing highly available systems.  </p><p>In the end, I moved away from CEPH and began to only maintain single instances of VMs and containers.  CEPH was nice to have but it limited the space I had available on the hard drives.  I kept the Proxmox nodes clustered together to maintain a single dashboard.</p><p>After my first system misconfiguration, I knew that I needed to implement a backup mechanism.  I bought a Synology NAS that I planned on using for backups utilizing NFS.  I backed up my data for a while this way until I came into contact with Proxmox Backup Server.  At first, I installed it on the Minisforum TH50, but I quickly realized that I did not have enough space to backup everything I wanted to and I wanted to ensure that my data was not lost if the used mini-pc quit working.  Then it dawned on me that I could run virtual machines on the Synology NAS.  I quickly spun up an instance of Proxmox Backup Server and pointed my Proxmox backups at it.  This is the backup solution that I continue to utilize.  </p><p>In the next post, I will talk about the next step in my journey and how I ended up buying way more hardware than I actually needed because I basically became addicted to growing my services and deploying enterprise solutions to my home lab.</p><p><strong>Update</strong></p><p>I was going to make a follow-up post for this article, but I have decided that instead of detailing how I built my home lab in the past tense, I am going to redesign it.  Currently, I have over 100 various Docker containers, LXC containers, and virtual machines.  There are several Docker containers that I spun up and thought I would use more than I currently am.  This is not the first complete redesign that I have gone through with my home lab and I am sure that it will not be the last.  As I find new services and learn how to use new tools, my home lab has grown outside of it's original bounds and I find that it is not optimally configured.  In the next couple of weeks, I will begin posting my design process and how I will build the services out.  Then I will begin posting about each service.  I want to document what the service does, the features that motivate me to use it, and the process I went through to implement it.</p>",
    "plaintext": "When I began building my home lab, I had no idea how much it would help me out in certifications and at work. I have been able to use my home lab as a testing server to learn how to setup and configure different apps and services. In this post and subsequent posts I will detail how I ended up becoming an avid homelabber and any advice that I have for others thinking about pursuing a home lab.\n\nIt all started two years ago with an install of Truenas Scale. The ability to run containers on my desktop computer that I converted into a NAS was what inspired me to go down the self-hosted rabbit hole. With these containers, I was introduced to Plex and Tailscale. The introduction to Plex inspired me to utilize other containers, but Tailscale was the real service that inspired my home lab. I discovered how easy it was to utilize and build Wireguard tunnels between my devices. It was this mobility that inspired me to try other services. If I had to be physically at home to use my home lab, I probably would not have made it this far.\n\nThe first hardware that I purchased for my home lab was an Intel NUC I bought for $50 on Ebay. Then, I began experimenting with different type-1(bare-metal) hypervisors. I started with Truenas Scale because it was what I was familiar with, but I tried several different hypervisors. In the end, I stuck with Proxmox because the GUI was organized in a way that made it easy to manage virtual machines and containers.\n\nOnce I discovered the ability to cluster Proxmox hosts, I bought another Intel NUC and a Minisforum TH50. I used the NUCs to create a Proxmox cluster. At first I had various services on each host, but then I began to experiment with CEPH storage and high-availability. It gave me some appreciation to the complexities involved with designing and implementing highly available systems.\n\nIn the end, I moved away from CEPH and began to only maintain single instances of VMs and containers. CEPH was nice to have but it limited the space I had available on the hard drives. I kept the Proxmox nodes clustered together to maintain a single dashboard.\n\nAfter my first system misconfiguration, I knew that I needed to implement a backup mechanism. I bought a Synology NAS that I planned on using for backups utilizing NFS. I backed up my data for a while this way until I came into contact with Proxmox Backup Server. At first, I installed it on the Minisforum TH50, but I quickly realized that I did not have enough space to backup everything I wanted to and I wanted to ensure that my data was not lost if the used mini-pc quit working. Then it dawned on me that I could run virtual machines on the Synology NAS. I quickly spun up an instance of Proxmox Backup Server and pointed my Proxmox backups at it. This is the backup solution that I continue to utilize.\n\nIn the next post, I will talk about the next step in my journey and how I ended up buying way more hardware than I actually needed because I basically became addicted to growing my services and deploying enterprise solutions to my home lab.\n\nUpdate\n\nI was going to make a follow-up post for this article, but I have decided that instead of detailing how I built my home lab in the past tense, I am going to redesign it. Currently, I have over 100 various Docker containers, LXC containers, and virtual machines. There are several Docker containers that I spun up and thought I would use more than I currently am. This is not the first complete redesign that I have gone through with my home lab and I am sure that it will not be the last. As I find new services and learn how to use new tools, my home lab has grown outside of it's original bounds and I find that it is not optimally configured. In the next couple of weeks, I will begin posting my design process and how I will build the services out. Then I will begin posting about each service. I want to document what the service does, the features that motivate me to use it, and the process I went through to implement it.",
    "feature_image": null,
    "excerpt": "When I began building my home lab, I had no idea how much it would help me out in certifications and at work. I have been able to use my home lab as a testing server to learn how to setup and configur...",
    "published_at": "2025-08-10T20:06:09.000Z",
    "published_date": "August 10, 2025",
    "tags": [
      "Home-Lab"
    ],
    "featured": false
  },
  {
    "id": "securityx-and-beyond",
    "title": "SecurityX and beyond",
    "slug": "securityx-and-beyond",
    "html": "<p>After my last post, I thought to myself why not go ahead and take the SecurityX exam.  I had a retest in case I failed, but I ended up not needing it.  I wanted to attempt the exam and get some idea of what the questions looked like.  I think that the momentum that is built by taking the lower level exams only makes the higher level exam easier.</p><p>I wasn't sure what to expect from the rebranded CASP+ exam, but it did not fail to disappoint.  Each question provided a scenario that guided you towards the answer.  There were questions that had several correct answer, but there was only one that fit the given scenario.  This exam also included the use of a virtual machine to test the hands-on ability of the test taker.</p><p>Personally, I thought that the Cysa+ and the Pentest+ exams were harder, but I think that is because of the specialized nature of them.  The SecurityX covers a broad range of topics and therefore has a large knowledge base to pull questions from.</p><p>When I decided that I wanted to switch careers, I enrolled in college to get a degree and made a plan to get the Security+.  I thought that the degree and Security+ would be a good start on getting an entry-level cyber security job.  After I started applying I realized that cyber security isn't really entry-level and that there is fierce competition for the available jobs.  Then I planned on getting maybe one more certification before I graduated.  Now I am graduating next Spring and am holding all of the CompTIA cyber security certifications.  </p><p>Honestly, I am not sure where to go from here.  I also want to get the OSCP, but I am unable to afford it right now.  I have been using the next level exam to renew the lower level certifications, but now that I have the highest cyber security exam I am going to have to learn how to submit CEUs.  I have been attending local and virtual conferences, so I am sure I will have no problem getting the CEUs.  I will just need to get into the habit of submitting them.</p>",
    "plaintext": "After my last post, I thought to myself why not go ahead and take the SecurityX exam. I had a retest in case I failed, but I ended up not needing it. I wanted to attempt the exam and get some idea of what the questions looked like. I think that the momentum that is built by taking the lower level exams only makes the higher level exam easier.\n\nI wasn't sure what to expect from the rebranded CASP+ exam, but it did not fail to disappoint. Each question provided a scenario that guided you towards the answer. There were questions that had several correct answer, but there was only one that fit the given scenario. This exam also included the use of a virtual machine to test the hands-on ability of the test taker.\n\nPersonally, I thought that the Cysa+ and the Pentest+ exams were harder, but I think that is because of the specialized nature of them. The SecurityX covers a broad range of topics and therefore has a large knowledge base to pull questions from.\n\nWhen I decided that I wanted to switch careers, I enrolled in college to get a degree and made a plan to get the Security+. I thought that the degree and Security+ would be a good start on getting an entry-level cyber security job. After I started applying I realized that cyber security isn't really entry-level and that there is fierce competition for the available jobs. Then I planned on getting maybe one more certification before I graduated. Now I am graduating next Spring and am holding all of the CompTIA cyber security certifications.\n\nHonestly, I am not sure where to go from here. I also want to get the OSCP, but I am unable to afford it right now. I have been using the next level exam to renew the lower level certifications, but now that I have the highest cyber security exam I am going to have to learn how to submit CEUs. I have been attending local and virtual conferences, so I am sure I will have no problem getting the CEUs. I will just need to get into the habit of submitting them.",
    "feature_image": null,
    "excerpt": "After my last post, I thought to myself why not go ahead and take the SecurityX exam. I had a retest in case I failed, but I ended up not needing it. I wanted to attempt the exam and get some idea of ...",
    "published_at": "2025-08-10T18:15:51.000Z",
    "published_date": "August 10, 2025",
    "tags": [
      "Personal Growth"
    ],
    "featured": false
  },
  {
    "id": "how-i-have-passed-certification-tests",
    "title": "How I have passed certification tests.",
    "slug": "how-i-have-passed-certification-tests",
    "html": "<p>I have a slightly unconventional way of taking certification exams from CompTIA.  I read every study guide related to cyber security before I decided that I wanted to pursue a career in cyber security.  Then I enrolled in college to get a bachelor's degree in cyber security.  I wanted to also get at least one certification before I graduated.  I thought that would help my resume stand out a little bit and also had a student discount when I was enrolled.  </p><p>Six months into college, I bought a voucher for the Network+ exam.  I scheduled that exam for two weeks later.  I felt like I was not prepared for the exam on test day, but I decided to go for it anyways.  I didn't get the highest score on the exam, but I passed it.  After I passed the Network+, I decided that I should ride that momentum and try to pass the Security+.  In the same month that I passed the Network+, I took and passed the Security+.  I think that taking the test with the Network+ information still fresh helped me on some of the Security+ questions.</p><p>Then time went on and I got a job in IT.  I was not as determined to get certifications at this point because I was going to school and getting some hands on experience at work.  Then I entered into my Senior year at college and realized that my discount was going to go away soon.  I bought the voucher for the Pentest+ and scheduled the exam for the day before my family was going on vacation to Gatlinburg, TN.  I knew if I passed it, I could ride that good feeling right into my vacation, but if I failed it, I might have spent my vacation thinking about what questions I got wrong.  I ended up passing it though.</p><p>After passing the Pentest+, I thought that I should go ahead and purchase the vouchers for the two other exams that I wanted to pass before I graduated.  I purchased the vouchers for the Cysa+ and the newly branded SecurityX (formerly CASP+).  I ended up buying both vouchers with a retake, so I thought screw it.  I will go ahead and take the Cysa+ exam the weekend after I got back from vacation, two weeks after I passed the Pentest+.  I also passed that exam and I think that building on the exams one after the other like that helped me to build my knowledge and to better retain it.</p><p>I passed the Cysa+ two weeks ago and now I scheduled the exam for the SecurityX for a little over a week away.  I am not overly confident that I will pass it, but I have been studying for it since before I started my cyber security journey and am currently making sure that I have all of the domains covered.  If I don't pass it, I will probably give myself about a month to study more and then take the exam again.</p><p>I spent a long time telling myself that I could not afford certifications, while also buying video games and systems.  Then I realized that buying the certifications was not wasting money on a piece of paper, but investing in my future.  I got a late start on my cyber security journey, so now I am doing everything I can to make up for it.</p>",
    "plaintext": "I have a slightly unconventional way of taking certification exams from CompTIA. I read every study guide related to cyber security before I decided that I wanted to pursue a career in cyber security. Then I enrolled in college to get a bachelor's degree in cyber security. I wanted to also get at least one certification before I graduated. I thought that would help my resume stand out a little bit and also had a student discount when I was enrolled.\n\nSix months into college, I bought a voucher for the Network+ exam. I scheduled that exam for two weeks later. I felt like I was not prepared for the exam on test day, but I decided to go for it anyways. I didn't get the highest score on the exam, but I passed it. After I passed the Network+, I decided that I should ride that momentum and try to pass the Security+. In the same month that I passed the Network+, I took and passed the Security+. I think that taking the test with the Network+ information still fresh helped me on some of the Security+ questions.\n\nThen time went on and I got a job in IT. I was not as determined to get certifications at this point because I was going to school and getting some hands on experience at work. Then I entered into my Senior year at college and realized that my discount was going to go away soon. I bought the voucher for the Pentest+ and scheduled the exam for the day before my family was going on vacation to Gatlinburg, TN. I knew if I passed it, I could ride that good feeling right into my vacation, but if I failed it, I might have spent my vacation thinking about what questions I got wrong. I ended up passing it though.\n\nAfter passing the Pentest+, I thought that I should go ahead and purchase the vouchers for the two other exams that I wanted to pass before I graduated. I purchased the vouchers for the Cysa+ and the newly branded SecurityX (formerly CASP+). I ended up buying both vouchers with a retake, so I thought screw it. I will go ahead and take the Cysa+ exam the weekend after I got back from vacation, two weeks after I passed the Pentest+. I also passed that exam and I think that building on the exams one after the other like that helped me to build my knowledge and to better retain it.\n\nI passed the Cysa+ two weeks ago and now I scheduled the exam for the SecurityX for a little over a week away. I am not overly confident that I will pass it, but I have been studying for it since before I started my cyber security journey and am currently making sure that I have all of the domains covered. If I don't pass it, I will probably give myself about a month to study more and then take the exam again.\n\nI spent a long time telling myself that I could not afford certifications, while also buying video games and systems. Then I realized that buying the certifications was not wasting money on a piece of paper, but investing in my future. I got a late start on my cyber security journey, so now I am doing everything I can to make up for it.",
    "feature_image": null,
    "excerpt": "I have a slightly unconventional way of taking certification exams from CompTIA. I read every study guide related to cyber security before I decided that I wanted to pursue a career in cyber security....",
    "published_at": "2025-08-05T19:50:26.000Z",
    "published_date": "August 05, 2025",
    "tags": [],
    "featured": false
  },
  {
    "id": "1st-post",
    "title": "1st Post",
    "slug": "1st-post",
    "html": "<p>This is my first blog post and honestly I don't know how I feel about it.  I am a more reserved person and have some social anxiety.  I have a hard time being comfortable around anyone, but I need to put myself out into the world.  I need to start living instead of just waking up, going through the motion, and going to sleep.  I also need to show my children that there is a large world out there and if they are determined enough, they can do anything they want.</p><p>I am a father of three children ages 11, 14, and 17.  I am in my mid-30's and woke up one day to realize that my life was slipping by.  I love my children and am proud of them, but I don't want them to be my only accomplishments.  I spent most of my adult life working in construction or operating a CNC machine in a small factory.  I dropped out of college to work full-time when my girlfriend was pregnant with our first child and I did work full-time.  I would say that on average over 15 years, I worked over 50 hours/week.  I think the constant work might have been what kept me from thinking about my dreams so much.  It wasn't really until I was talking with my oldest daughter about college that I realized, I had my own aspirations.  They had been quietly sitting back there all of this time, just waiting for me to get the \"time\" to pursue them.</p><p>At that moment, I enrolled on college online at Champlain College and began the pursuit of my bachelor's degree in cyber security.  I wasn't sure at the time if certifications or a degree were the smart bet, so I decided to get both.  I have been getting all the certifications that I can afford and I absolutely love it.  I forgot how much I enjoyed just learning.  I realize that a lot of the certifications are not hands on and to help make up for that I began building a home lab.  I also began doing modules on the Hack the Box Academy. </p><p>I am just building out what I want this blog to be, but I think I am going to divide it into sections.  I want one section that I use for home lab projects, one that details cyber security related home lab projects, one that talks about the different certifications processes, and possible one were I talking about being a father of three children while pursuing these projects.  I am not sure how I feel about the last one, but I think it could be helpful for someone to hear that they are not the only one struggling to balance family, work, and life. </p>",
    "plaintext": "This is my first blog post and honestly I don't know how I feel about it. I am a more reserved person and have some social anxiety. I have a hard time being comfortable around anyone, but I need to put myself out into the world. I need to start living instead of just waking up, going through the motion, and going to sleep. I also need to show my children that there is a large world out there and if they are determined enough, they can do anything they want.\n\nI am a father of three children ages 11, 14, and 17. I am in my mid-30's and woke up one day to realize that my life was slipping by. I love my children and am proud of them, but I don't want them to be my only accomplishments. I spent most of my adult life working in construction or operating a CNC machine in a small factory. I dropped out of college to work full-time when my girlfriend was pregnant with our first child and I did work full-time. I would say that on average over 15 years, I worked over 50 hours/week. I think the constant work might have been what kept me from thinking about my dreams so much. It wasn't really until I was talking with my oldest daughter about college that I realized, I had my own aspirations. They had been quietly sitting back there all of this time, just waiting for me to get the \"time\" to pursue them.\n\nAt that moment, I enrolled on college online at Champlain College and began the pursuit of my bachelor's degree in cyber security. I wasn't sure at the time if certifications or a degree were the smart bet, so I decided to get both. I have been getting all the certifications that I can afford and I absolutely love it. I forgot how much I enjoyed just learning. I realize that a lot of the certifications are not hands on and to help make up for that I began building a home lab. I also began doing modules on the Hack the Box Academy.\n\nI am just building out what I want this blog to be, but I think I am going to divide it into sections. I want one section that I use for home lab projects, one that details cyber security related home lab projects, one that talks about the different certifications processes, and possible one were I talking about being a father of three children while pursuing these projects. I am not sure how I feel about the last one, but I think it could be helpful for someone to hear that they are not the only one struggling to balance family, work, and life.",
    "feature_image": null,
    "excerpt": "This is my first blog post and honestly I don't know how I feel about it. I am a more reserved person and have some social anxiety. I have a hard time being comfortable around anyone, but I need to pu...",
    "published_at": "2025-07-12T23:09:43.000Z",
    "published_date": "July 12, 2025",
    "tags": [
      "Personal Growth"
    ],
    "featured": false
  }
]